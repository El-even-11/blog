[{"content":"完结撒花！临近期末周了，来摸摸鱼记录一下 Bustub Concurrency Control 的实现过程。\n非常感谢 CMU 慷慨地开源如此优质的课程。\nResources https://15445.courses.cs.cmu.edu/fall2022 课程官网 https://github.com/cmu-db/bustub Bustub Github Repo https://www.gradescope.com/ 自动测评网站 GradeScope，course entry code: PXWVR5 https://discord.gg/YF7dMCg Discord 论坛，课程交流用 bilibili 有搬运的课程视频，自寻。 请不要将实现代码公开，尊重 Andy 和 TAs 的劳动成果！\nOverview Project 4 是 15-445 2022Fall 的最后一个部分了，在这里我们将为 Bustub 实现关系数据库中极其重要的 transaction 概念。难度上来说个人感觉 Project 2 \u0026raquo; 4 \u0026gt; 3 ≈ 1。Project 4 实现难度还是不小的，分为三个部分：\nLock Manager：锁管理器，利用 2PL 实现并发控制。支持 REPEATABLE_READ、READ_COMMITTED 和 READ_UNCOMMITTED 三种隔离级别，支持 SHARED、EXCLUSIVE、INTENTION_SHARED、INTENTION_EXCLUSIVE 和 SHARED_INTENTION_EXCLUSIVE 五种锁，支持 table 和 row 两种锁粒度，支持锁升级。Project 4 重点部分。 Deadlock Detection：死锁检测，运行在一个 background 线程，每间隔一定时间检测当前是否出现死锁，并挑选合适的事务将其 abort 以解开死锁。 Concurrent Query Execution：修改之前实现的 SeqScan、Insert 和 Delete 算子，加上适当的锁以实现并发的查询。 Task 1 Lock Manager 大部分需要注意的内容都在 lock_manager.h 的 LOCK NOTE 和 UNLOCK NOTE 里了。在开始实现前务必阅读。\n首先理一理 Lock Manager 的结构：\ntable_lock_map_：记录 table 和与其相关锁请求的映射关系。 row_lock_map_：记录 row 和与其相关锁请求的映射关系。 这两个 map 的值均为锁请求队列 LockRequestQueue：\nrequest_queue_：实际存放锁请求的队列。 cv_ \u0026amp; latch_：条件变量和锁，配合使用可以实现经典的等待资源的模型。 upgrading_：正在此资源上尝试锁升级的事务 id。 锁请求以 LockRequest 类表示：\ntxn_id_：发起此请求的事务 id。 lock_mode_：请求锁的类型。 oid_：在 table 粒度锁请求中，代表 table id。在 row 粒度锁请求中，表示 row 属于的 table 的 id。 rid_：仅在 row 粒度锁请求中有效。指 row 对应的 rid。 granted_：是否已经对此请求授予锁？ Lock Manager 的作用是处理事务发送的锁请求，例如有一个 SeqScan 算子需要扫描某张表，其所在事务就需要对这张表加 S 锁。而加读锁这个动作需要由 Lock Manager 来完成。事务先对向 Lock Manager 发起加 S 锁请求，Lock Manager 对请求进行处理。如果发现此时没有其他的锁与这个请求冲突，则授予其 S 锁并返回。如果存在冲突，例如其他事务持有这张表的 X 锁，则 Lock Manager 会阻塞此请求（即阻塞此事务），直到能够授予 S 锁，再授予并返回。\n现在可能还有点迷糊，接下来理一理 lock 和 unlock 的整个过程应该就比较清晰了。\nLock 以 table lock 为例。首先需要注意，Project 4 中细节极多，每次操作都有很多状态需要同步和维护，因此尽量遵循防御性编程的理念，多用 assert，确保系统处于预期的状态。\n第一步，检查 txn 的状态。\n若 txn 处于 Abort/Commit 状态，抛逻辑异常，不应该有这种情况出现。\n若 txn 处于 Shrinking 状态，则需要检查 txn 的隔离级别和当前锁请求类型：\nREPEATABLE_READ: The transaction is required to take all locks. All locks are allowed in the GROWING state No locks are allowed in the SHRINKING state READ_COMMITTED: The transaction is required to take all locks. All locks are allowed in the GROWING state Only IS, S locks are allowed in the SHRINKING state READ_UNCOMMITTED: The transaction is required to take only IX, X locks. X, IX locks are allowed in the GROWING state. S, IS, SIX locks are never allowed 三种隔离级别翻译成中文分别是可重复读、读已提交、读未提交。另外 lecture 中一直提到的 SERIALIZABLE 可串行化隔离级别在 Project 4 中无需实现。实现 SERIALIZABLE 需要在 REPEATABLE_READS 的基础上加上 index lock。这里我们无需关注 index lock。\n以下是 15-445 Lecture 17 slides 中的截图，介绍了不同隔离级别下可能出现的异常情况：\n不同的隔离级别其实是性能与一致性的权衡。关于隔离级别的更多内容可以看看 15-445 的 Lecture。\n在 Project 4 中仅需支持除了 SERIALIZABLE 外的剩下三种隔离级别。若 txn 处于 Shrinking 状态：\n在 REPEATABLE_READ 下，造成事务终止，并抛出 LOCK_ON_SHRINKING 异常。 在 READ_COMMITTED 下，若为 IS/S 锁，则正常通过，否则抛 LOCK_ON_SHRINKING。 在 READ_UNCOMMITTED 下，若为 IX/X 锁，抛 LOCK_ON_SHRINKING，否则抛 LOCK_SHARED_ON_READ_UNCOMMITTED。 若 txn 处于 Growing 状态，若隔离级别为 READ_UNCOMMITTED 且锁类型为 S/IS/SIX，抛 LOCK_SHARED_ON_READ_UNCOMMITTED。其余状态正常通过。\n第一步保证了锁请求、事务状态、事务隔离级别的兼容。正常通过第一步后，可以开始尝试获取锁。\n第二步，获取 table 对应的 lock request queue。\n从 table_lock_map_ 中获取 table 对应的 lock request queue。注意需要对 map 加锁，并且为了提高并发性，在获取到 queue 之后立即释放 map 的锁。若 queue 不存在则创建。\n第三步，检查此锁请求是否为一次锁升级。\n首先，记得对 queue 加锁。\ngranted 和 waiting 的锁请求均放在同一个队列里，我们需要遍历队列查看有没有与当前事务 id（我习惯叫做 tid）相同的请求。如果存在这样的请求，则代表当前事务在此前已经得到了在此资源上的一把锁，接下来可能需要锁升级。需要注意的是，这个请求的 granted_ 一定为 true。因为假如事务此前的请求还没有被通过，事务会被阻塞在 LockManager 中，不可能再去尝试获取另一把锁。\n现在我们找到了此前已经获取的锁，开始尝试锁升级。首先，判断此前授予锁类型是否与当前请求锁类型相同。若相同，则代表是一次重复的请求，直接返回。否则进行下一步检查。\n接下来，判断当前资源上是否有另一个事务正在尝试升级（queue-\u0026gt;upgrading_ == INVALID_TXN_ID）。若有，则终止当前事务，抛出 UPGRADE_CONFLICT 异常。因为不允许多个事务在同一资源上同时尝试锁升级。\n[LOCK UPGRADE] Furthermore, only one transaction should be allowed to upgrade its lock on a given resource. Multiple concurrent lock upgrades on the same resource should set the TransactionState as ABORTED and throw a TransactionAbortException (UPGRADE_CONFLICT).\n为什么不允许？我忘了。。。我记得在实现的过程中突然灵感一闪，领悟出为何不允许多事务同时进行锁升级（也可能是当时就想错了），但由于没有及时记录，时间间隔有点久，现在想不起来了。之后再慢慢想想到底是为了降低实现复杂度还是有什么其他的特殊考量。\n然后，判断升级锁的类型和之前锁是否兼容，不能反向升级。\nWhile upgrading, only the following transitions should be allowed: IS -\u0026gt; [S, X, IX, SIX] S -\u0026gt; [X, SIX] IX -\u0026gt; [X, SIX] SIX -\u0026gt; [X]\n若不兼容，抛 INCOMPATIBLE_UPGRADE 异常。\n现在，我们终于可以进行锁升级了。引用 Discord 里 15-445 TA 的原话，锁升级可以被拆分成三个步骤：\n可以升级吗？即我们此前的一系列判断。 释放当前已经持有的锁，并在 queue 中标记我正在尝试升级。 等待直到新锁被授予。 需要特别注意的是，在锁升级时，需要先释放此前持有的锁，把升级作为一个新的请求加入队列。之前我以为在锁升级时要一直持有此前的锁，直到能够升级时，再直接修改锁的类型。按此实现之后被一个 test case 卡到怀疑人生。\n锁升级的步骤大概就是这样。当然，假如遍历队列后发现不存在与当前 tid 相同的请求，就代表这是一次平凡的锁请求。\n第四步，将锁请求加入请求队列。\nnew 一个 LockRequest，加入队列尾部。实际上这里 LockRequest 不应该设计成裸指针，不方便管理，之后要记得手动释放。最好用智能指针。没用智能指针的原因是忘了（不过完全可以自己改成智能指针。\n这里采用一条队列，把 granted 和 waiting 的请求放在一起，个人感觉不是特别清晰。或许可以改成一条 granted 队列和一条 waiting 队列。\n第五步，尝试获取锁。\n这是最后一步，也是最核心的一步，体现了 Lock Manager 的执行模型。首先，需要清楚条件变量的使用场景。\n条件变量并不是某一个特定语言中的概念，而是操作系统中线程同步的一种机制。先给出条件变量经典的使用形式：\nstd::unique_lock\u0026lt;std::mutex\u0026gt; lock(latch); while (!resource) { cv.wait(lock); } 条件变量与互斥锁配合使用。首先需要持有锁，并查看是否能够获取资源。这个锁与资源绑定，是用来保护资源的锁。若暂时无法获取资源，则调用条件变量的 wait 函数。调用 wait 函数后，latch 将自动释放，并且当前线程被挂起，以节省资源。这就是阻塞的过程。此外，允许有多个线程在 wait 同一个 latch。\n当其他线程的活动使得资源状态发生改变时，需要调用条件遍历的 notify_all() 函数。即\n// do something changing the state of resource... cv.notify_all(); notify_all() 可以看作一次广播，会唤醒所有正在此条件变量上阻塞的线程。在线程被唤醒后，其仍处于 wait 函数中。在 wait 函数中尝试获取 latch。在成功获取 latch 后，退出 wait 函数，进入循环的判断条件，检查是否能获取资源。若仍不能获取资源，就继续进入 wait 阻塞，释放锁，挂起线程。若能获取资源，则退出循环。这样就实现了阻塞等待资源的模型。条件变量中的条件指的就是满足某个条件，在这里即能够获取资源。\n理解条件变量的作用后，就可以写出如下代码：\nstd::unique_lock\u0026lt;std::mutex\u0026gt; lock(queue-\u0026gt;latch_); while (!GrantLock(...)) { queue-\u0026gt;cv_.wait(lock); } 在 GrantLock() 中，Lock Manager 会判断是否可以满足当前锁请求。若可以满足，则返回 true，事务成功获取锁，并退出循环。若不能满足，则返回 false，事务暂时无法获取锁，在 wait 处阻塞，等待资源状态变化时被唤醒并再次判断是否能够获取锁。资源状态变化指的是什么？其他事务释放了锁。\n接下来是 GrantLock() 函数。在此函数中，我们需要判断当前锁请求是否能被满足。\n判断兼容性。遍历请求队列，查看当前锁请求是否与所有的已经 granted 的请求兼容。需要注意的是，在我的实现中 granted 请求不一定都在队列头部，因此需要完全遍历整条队列。锁兼容矩阵可以在 Lecture slides 中查看。若全部兼容，则通过检查。否则直接返回 false。当前请求无法被满足。\n判断优先级。锁请求会以严格的 FIFO 顺序依次满足。只有当前请求为请求队列中优先级最高的请求时，才允许授予锁。优先级可以这样判断：\n如果队列中存在锁升级请求，若锁升级请求正为当前请求，则优先级最高。否则代表其他事务正在尝试锁升级，优先级高于当前请求。 若队列中不存在锁升级请求，则遍历队列。如果，当前请求是第一个 waiting 状态的请求，则代表优先级最高。如果当前请求前面还存在其他 waiting 请求，则要判断当前请求是否前面的 waiting 请求兼容。若兼容，则仍可以视为优先级最高。若存在不兼容的请求，则优先级不为最高。 这么说可能比较抽象，举几个具体的例子：\nX(waiting) X(upgrading) ^ current Current request is upgrading. Highest priority. -------------------------------------------------- X(waiting) S(waiting) ^ current Current request is incompatible with the first request. Not highest priority. -------------------------------------------------- S(waiting) S(waiting) ^ current Although current request is not the first waiting request, it is compatible with the first request. Highest priority. 其他的情况都比较好理解，遵循升级请求优先级最高和 FIFO 的原则。但最后一种情况可能稍微不太好理解。之所以要这么做是因为 LOCK NOTE 里有这么一句话：\nIf there are multiple compatible lock requests, all should be granted at the same time as long as FIFO is honoured.\n所有兼容的锁请求需要一起被授予。\n两项检查通过后，代表当前请求既兼容又有最高优先级，因此可以授予锁。授予锁的方式是将 granted_ 置为 true。并返回 true。假如这是一次升级请求，则代表升级完成，还要记得将 upgrading_ 置为 INVALID_TXN_ID。\n另外，需要进行一些 Bookkeeping 操作。Transaction 中需要维护许多集合，分别记录了 Transaction 当前持有的各种类型的锁。方便在事务提交或终止后全部释放。\nLock 的流程大致如此，row lock 与 table lock 几乎相同，仅多了一个检查步骤。在接收到 row lock 请求后，需要检查是否持有 row 对应的 table lock。必须先持有 table lock 再持有 row lock。\nUnlock 仍以 table lock 为例。Unlock 的流程比 Lock 要简单不少。\n首先，由于是 table lock，在释放时需要先检查其下的所有 row lock 是否已经释放。\n接下来是 table lock 和 row lock 的公共步骤：\n第一步，获取对应的 lock request queue。\n第二步，遍历请求队列，找到 unlock 对应的 granted 请求。\n若不存在对应的请求，抛 ATTEMPTED_UNLOCK_BUT_NO_LOCK_HELD 异常。\n找到对应的请求后，根据事务的隔离级别和锁类型修改其状态。\n当隔离级别为 REPEATABLE_READ 时，S/X 锁释放会使事务进入 Shrinking 状态。当为 READ_COMMITTED 时，只有 X 锁释放使事务进入 Shrinking 状态。当为 READ_UNCOMMITTED 时，X 锁释放使事务 Shrinking，S 锁不会出现。\n之后，在请求队列中 remove unlock 对应的请求，并将请求 delete。\n同样，需要进行 Bookkeeping。\n在锁成功释放后，调用 cv_.notify_all() 唤醒所有阻塞在此 table 上的事务，检查能够获取锁。\nTask 1 的内容就是这样，核心是条件变量阻塞模型，此外细枝末节还是挺多的，需要细心维护。\nTask 2 Deadlock Detection 在阻塞过程中有可能会出现多个事务的循环等待，而循环等待会造成死锁。在 Bustub 中我们采用一个 background Deadlock Detection 线程来定时检查当前是否出现死锁。\n我们用 wait for 图来表示事务之间的等待关系。wait for 是一个有向图，t1-\u0026gt;t2 即代表 t1 事务正在等待 t2 事务释放资源。当 wait for 图中存在环时，即代表出现死锁，需要挑选事务终止以打破死锁。\n我们并不需要时刻维护 wait for 图，而是在死锁检测线程被唤醒时，根据当前请求队列构建 wait for 图，再通过 wait for 图判断是否存在死锁。当判断完成后，将丢弃当前 wait for 图。下次线程被唤醒时再重新构建。\n最常见的有向图环检测算法包括 DFS 和拓扑排序。在这里我们选用 DFS 来进行环检测。构建 wait for 图时要保证搜索的确定性。始终从 tid 较小的节点开始搜索，在选择邻居时，也要优先搜索 tid 较小的邻居。\n构建 wait for 图的过程是，遍历 table_lock_map 和 row_lock_map 中所有的请求队列，对于每一个请求队列，用一个二重循环将所有满足等待关系的一对 tid 加入 wait for 图的边集。满足等待关系是指，例如二元组 (a, b)，a 是 waiting 请求，b 是 granted 请求，并且 a 与 b 的锁不兼容，则生成 a-\u0026gt;b 一条边。\n在成功构建 wait for 图后，对 wait for 图实施环检测算法。具体的算法就不说了，正好可以复习复习有向图环检测。有一点需要注意，这个环检测算法不仅需要输出是否存在环，假如存在环，还要输出环上的所有节点。因为之后我们需要在这些成环的节点里挑选合适的事务进行终止。\n在发现环后，我们可以得到环上的所有节点。此时我们挑选 youngest 的事务将其终止。这里的 youngest 如何理解呢？一开始我以为是事务到达的时间，因此我维护了一张 map，记录了每个事务最后一次发起锁请求的逻辑时间戳，挑选时间戳最大的事务终止。在提交测试后我懵了。有一个死锁检测 test case 在返回需要 abort 的事务时，一直返回 -1（INVALID_TXN_ID）。这个测试会绕过 Lock Manager 直接构建 wait for 图来验证是否终止了 youngest 的事务。因此，我又成小丑了，实际上只用挑选 tid 最大的事务作为 youngest 事务终止即可。\n挑选出 youngest 事务后，将此事务的状态设为 Aborted。并且在请求队列中移除此事务，释放其持有的锁，终止其正在阻塞的请求，并调用 cv_.notify_all() 通知正在阻塞的相关事务。此外，还需移除 wait for 图中与此事务有关的边。不是不用维护 wait for 图，每次使用重新构建吗？这是因为图中可能存在多个环，不是打破一个环就可以直接返回了。需要在死锁检测线程醒来的时候打破当前存在的所有环。\n写到这里，不难发现我们之前的阻塞模型需要进行一定的修改：\nstd::unique_lock\u0026lt;std::mutex\u0026gt; lock(queue-\u0026gt;latch_); while (!GrantLock(...)) { queue-\u0026gt;cv_.wait(lock); if (txn-\u0026gt;GetState() == Aborted) { // release resources return false; } } 在事务被唤醒时，其可能已经被终止掉了。原因可能是死锁检测中将其终止，也可能是外部的一些原因造成终止。因此需要检测是否处于 Aborted 状态，若处于则释放所持资源并返回。\nTask 2 的内容就是这些，核心是环检测算法。\nTask 3 Concurrent Query Execution 这一部分需要我们将 transaction 应用到之前实现的算子中，以支持并发的查询。比较简单。\n我们仅需修改 SeqScan、Insert 和 Delete 三个算子。为什么其他的算子不需要修改？因为其他算子获取的 tuple 数据均为中间结果，并不是表中实际的数据。而这三个算子是需要与表中实际数据打交道的。其中 Insert 和 Delete 几乎完全一样，与 SeqScan 分别代表着写和读。\nSeqScan 在 Init() 函数中，为表加上 S 锁。如果隔离级别是 READ_UNCOMMITTED 则无需加锁。加锁失败则抛出 ExecutionException 异常。\n在 Next() 函数中，若表中已经没有数据，则释放之前持有的锁。\nInsert \u0026amp; Delete 在 Init() 函数中，为表加上 X 锁。实际上为表加 IX 锁再为行加 X 锁应该也可以，我这里图省事就直接用大锁了。同样，若获取失败则抛 ExecutionException 异常。另外，这里的获取失败不仅是结果返回 false，还有可能是抛出了 TransactionAbort() 异常，例如 UPGRADE_CONFLICT，需要用 try catch 捕获。\n在 Next() 函数需要返回 false 前，释放持有的 X 锁。\n另外，在 Notes 里提到的\nyou will need to maintain the write sets in transactions\n似乎 InsertTuple() 函数里已经帮我们做好了，不需要维护 write set。而 index 的 write set 需要我们自己维护。当然，Project 4 并不需要考虑 index，不维护代码也能过。\nSummary 这就是 Project 4 或者说 CMU15-445 2022Fall Projects 的全部内容了。在 Project 4 里手写了一个锁管理器，为 Bustub 提供以 2PL 为基础的并发查询和事务模型，收获同样很大。另外老实说由于快到期末周了，这篇记录写的比较仓促，没有讨论太多设计的理念，仅仅是记录了一下实现过程，写的比较水，还请见谅。\n整个 15-445 Projects 做下来花了差不多 150 个小时的时间，还仅仅是编码时间，不包括看 Lecture 等等，但做完这门课我的收获可能比在某高校学习的所有课加起来还大。真正地完成了我对数据库领域的启蒙教育。\n感谢 CMU 慷慨地提供如此优质的教学资源，开放了 AutoGrader 测评，感谢 Andy 的教学，感谢 15-445 TAs 在 non-CMU Discord 里对各种问题的解答。\n最后，写下这个系列的初衷是为了自己能够更好地理解知识，复盘实现过程，以后完全看不懂自己曾经写了啥的时候也可以来看看当时的想法。另外，假如这些能够为朋友们或后来者提供一些微小的帮助，便是再好不过了。\n","permalink":"https://blog.eleven.wiki/posts/cmu15-445-project4-concurrency-control/","summary":"完结撒花！临近期末周了，来摸摸鱼记录一下 Bustub Concurrency Control 的实现过程。\n非常感谢 CMU 慷慨地开源如此优质的课程。\nResources https://15445.courses.cs.cmu.edu/fall2022 课程官网 https://github.com/cmu-db/bustub Bustub Github Repo https://www.gradescope.com/ 自动测评网站 GradeScope，course entry code: PXWVR5 https://discord.gg/YF7dMCg Discord 论坛，课程交流用 bilibili 有搬运的课程视频，自寻。 请不要将实现代码公开，尊重 Andy 和 TAs 的劳动成果！\nOverview Project 4 是 15-445 2022Fall 的最后一个部分了，在这里我们将为 Bustub 实现关系数据库中极其重要的 transaction 概念。难度上来说个人感觉 Project 2 \u0026raquo; 4 \u0026gt; 3 ≈ 1。Project 4 实现难度还是不小的，分为三个部分：\nLock Manager：锁管理器，利用 2PL 实现并发控制。支持 REPEATABLE_READ、READ_COMMITTED 和 READ_UNCOMMITTED 三种隔离级别，支持 SHARED、EXCLUSIVE、INTENTION_SHARED、INTENTION_EXCLUSIVE 和 SHARED_INTENTION_EXCLUSIVE 五种锁，支持 table 和 row 两种锁粒度，支持锁升级。Project 4 重点部分。 Deadlock Detection：死锁检测，运行在一个 background 线程，每间隔一定时间检测当前是否出现死锁，并挑选合适的事务将其 abort 以解开死锁。 Concurrent Query Execution：修改之前实现的 SeqScan、Insert 和 Delete 算子，加上适当的锁以实现并发的查询。 Task 1 Lock Manager 大部分需要注意的内容都在 lock_manager.","title":"CMU15-445 Project4 Concurrency Control"},{"content":"来记录一下 Bustub Query Execution 的实现过程。\n在阅读本文前，墙裂推荐阅读 Project 3 开发者迟先生的这篇文章： BusTub 养成记：从课程项目到 SQL 数据库 可以更清晰地了解到 Bustub SQL 层的设计过程。\nResources https://15445.courses.cs.cmu.edu/fall2022 课程官网 https://github.com/cmu-db/bustub Bustub Github Repo https://www.gradescope.com/ 自动测评网站 GradeScope，course entry code: PXWVR5 https://discord.gg/YF7dMCg Discord 论坛，课程交流用 bilibili 有搬运的课程视频，自寻。 https://15445.courses.cs.cmu.edu/fall2022/bustub/ 在你的浏览器上运行 Bustub！ 请不要将实现代码公开，尊重 Andy 和 TAs 的劳动成果！\nOverview Andy 在 Lecture 中说，Query Optimization 是数据库最难的部分，Transaction 是第二难的部分。总体来说，Project 3 的难度不算大，但和 Project 2 的难点恰好相反：Project 2 的难点在于从零实现 B+ 树，一切都得靠自己。Project 3 的难点在于读代码，理解查询引擎的原理，具体实现起来其实比较简单。\n这是课程官网的一张图，清晰地介绍了 Bustub 的整体架构。在 Project 3 中，我们需要实现一系列 Executors，以及为 Optimizer 添加新功能。\nTask1：Access Method Executors. 包含 SeqScan、Insert、Delete、IndexScan 四个算子。 Task2：Aggregation and Join Executors. 包含 Aggregation、NestedLoopJoin、NestedIndexJoin 三个算子。 Task3：Sort + Limit Executors and Top-N Optimization. 包含 Sort、Limit、TopN 三个算子，以及实现将 Sort + Limit 优化为 TopN 算子。 Leaderboard Task：为 Optimizer 实现新的优化规则，包括 Hash Join、Join Reordering、Filter Push Down、Column Pruning 等等，让三条诡异的 sql 语句执行地越快越好。 Talking Casually 在正式开始记录 Project 3 的具体实现之前，我想随便聊聊 Bustub 整体的结构与运行流程。在迷迷糊糊地通过 Project 3 的所有 tests 后，我意识到这其实是了解数据库到底是如何执行 sql 语句的最佳时机。Project 1\u0026amp;2 都比较局部，而在这里，一个能真正执行 sql 语句的数据库已经构建起来了。先暂时抛开 transaction，来看看一条 sql 语句在 Bustub 中的旅行。\nParser 一条 sql 语句，首先经过 Parser 生成一棵抽象语法树 AST。具体如何生成，请参考编译原理。Parser 不是数据库的核心部分，也不是性能瓶颈，因此除非热爱编译原理，或者想通过实现一个 sql Parser 对编译原理进行实践，否则一般都会采用第三方库。Bustub 中采用了 libpg_query 库将 sql 语句 parse 为 AST。\nBinder 在得到 AST 后，还需要将这些词语绑定到数据库实体上，这就是 Binder 的工作。例如有这样一条 sql：\nSELECT colA FROM table1; 其中 SELECT 和 FROM 是关键字，colA 和 table1 是标识符。Binder 遍历 AST，将这些词语绑定到相应的实体上。实体是 Bustub 可以理解的各种 c++ 类。绑定完成后，得到的结果是一棵 Bustub 可以直接理解的树。把它叫做 Bustub AST。\nPlanner 得到 Bustub AST 后，Planner 遍历这棵树，生成初步的查询计划。查询计划也是一棵树的形式。例如这条 sql：\nSELECT t1.y, t2.x FROM t1 INNER JOIN t2 ON t1.x = t2.y; 对应的原始的查询计划是\n查询计划规定了数据的流向。数据从树叶流向树根，自底向上地流动，在根节点输出结果。\nOptimizer 由 Planner 得到初步的查询计划后，再将查询计划交给 Optimizer 进行修改优化，生成优化过后的最终查询计划。Optimizer 主要有两种实现方式：\nRule-based. Optimizer 遍历初步查询计划，根据已经定义好的一系列规则，对 PlanNode 进行一系列的修改、聚合等操作。例如我们在 Task 3 中将要实现的，将 Limit + Sort 合并为 TopN。这种 Optimizer 不需要知道数据的具体内容，仅是根据预先定义好的规则修改 Plan Node。 Cost-based. 这种 Optimizer 首先需要读取数据，利用统计学模型来预测不同形式但结果等价的查询计划的 cost。最终选出 cost 最小的查询计划作为最终的查询计划。 Bustub 的 Optimizer 采用第一种实现方式。MIT6.830 的 SimpleDB 则是采用第二种方式，有兴趣也可以看看。\n另外值得一提的是，一般来说，Planner 生成的是 Logical Plan Node，代表抽象的 Plan。Optimizer 则生成 Physical Plan Node，代表具体执行的 Plan。一个比较典型的例子是 Join。在 Planner 生成的查询计划中，Join 就是 Join。在 Optimizer 生成的查询计划中，Join 会被优化成具体的 HashJoin 或 NestedIndexJoin 等等。在 Bustub 中，并不区分 Logical Plan Node 和 Physical Plan Node。Planner 会直接生成 Physical Plan Node。\nExecutor 在拿到 Optimizer 生成的具体的查询计划后，就可以生成真正执行查询计划的一系列算子了。算子也是我们在 Project 3 中需要实现的主要内容。生成算子的步骤很简单，遍历查询计划树，将树上的 PlanNode 替换成对应的 Executor。算子的执行模型也大致分为三种：\nIterator Model，或 Pipeline Model，或火山模型。每个算子都有 Init() 和 Next() 两个方法。Init() 对算子进行初始化工作。Next() 则是向下层算子请求下一条数据。当 Next() 返回 false 时，则代表下层算子已经没有剩余数据，迭代结束。可以看到，火山模型一次调用请求一条数据，占用内存较小，但函数调用开销大，特别是虚函数调用造成 cache miss 等问题。 Materialization Model. 所有算子立即计算出所有结果并返回。和 Iterator Model 相反。这种模型的弊端显而易见，当数据量较大时，内存占用很高。但减少了函数调用的开销。比较适合查询数据量较小的 OLTP workloads。 Vectorization Model. 对上面两种模型的中和，一次调用返回一批数据。利于 SIMD 加速。目前比较先进的 OLAP 数据库都采用这种模型。 Bustub 采用的是 Iterator Model。\n此外，算子的执行方向也有两种：\nTop-to-Bottom. 从根节点算子开始，不断地 pull 下层算子的数据。 Bottom-to-Top. 从叶子节点算子开始，向上层算子 push 自己的数据。 Bustub 采用 Top-to-Bottom。\n在根节点算子处，就得到了我们想要查询的数据，一条 sql 语句完成了它的使命。\n另外，我们在 Project 1 中实现的 Buffer Pool 和在 Project 2 中实现的 B+Tree Index 在哪里？实际上就在一系列算子下。例如 SeqScan 算子，需要遍历 table，首先通过数据库的 catalog 找到对应的 table，一个 table 由许多 page 组成，在访问 page 时，就用到了 Buffer Pool。在 Optimizer 中，假如发现 Sort 算子在对 indexed attribute 排序，会将 Sort 算子优化为 IndexScan 算子，这样就用到了 B+Tree Index。\nBustub Query Execution 的大致结构就是这样，还有很多设计上的细节没有提到，比如 Tuple、Value、AbstractExpression 等等。接下来在具体实现中边看边聊。\nTask 1 Access Method Executors Task 1 包含 4 个算子，SeqScan、Insert、Delete 和 IndexScan。\nSeqScan 读取给定 table 中的所有 tuple，仅会出现在查询计划的叶子节点处。直接使用已经提供的 TableIterator。实现起来挺简单的。此外主要想聊聊 Bustub 中 table 的结构。\n首先，Bustub 有一个 Catalog。Catalog 提供了一系列 API，例如 CreateTable()、GetTable() 等等。Catalog 维护了几张 hashmap，保存了 table id 和 table name 到 table info 的映射关系。table id 由 Catalog 在新建 table 时自动分配，table name 则由用户指定。\n这里的 table info 包含了一张 table 的 metadata，有 schema、name、id 和指向 table heap 的指针。系统的其他部分想要访问一张 table 时，先使用 name 或 id 从 Catalog 得到 table info，再访问 table info 中的 table heap。\ntable heap 是管理 table 数据的结构，包含 InsertTuple()、MarkDelete() 一系列 table 相关操作。table heap 本身并不直接存储 tuple 数据，tuple 数据都存放在 table page 中。table heap 可能由多个 table page 组成，仅保存其第一个 table page 的 page id。需要访问某个 table page 时，通过 page id 经由 buffer pool 访问。\ntable page 是实际存储 table 数据的结构，父类是 page。相较于 page，table page 多了一些新的方法。table page 在 data 的开头存放了 next page id、prev page id 等信息，将多个 table page 连成一个双向链表，便于整张 table 的遍历操作。当需要新增 tuple 时，table heap 会找到当前属于自己的最后一张 table page，尝试插入，若最后一张 table page 已满，则新建一张 table page 插入 tuple。table page 低地址存放 header，tuple 从高地址也就是 table page 尾部开始插入。\ntuple 对应数据表中的一行数据。每个 tuple 都由 RID 唯一标识。RID 由 page id + slot num 构成。tuple 由 value 组成，value 的个数和类型由 table info 中的 schema 指定。\nvalue 则是某个字段具体的值，value 本身还保存了类型信息。\n需要注意的是，executor 本身并不保存查询计划的信息，应该通过 executor 的成员 plan 来得知该如何进行本次计算，例如 SeqScanExecutor 需要向 SeqScanPlanNode 询问自己该扫描哪张表。\n所有要用到的系统资源，例如 Catalog，Buffer Pool 等，都由 ExecutorContext 提供。\nInsert \u0026amp; Delete Insert 和 Delete 这两个算子实现起来基本一样，也比较特殊，是唯二的写算子。数据库最主要的操作就是增查删改。Bustub sql 层暂时不支持 UPDATE。Insert 和 Delete 一定是查询计划的根节点，且仅需返回一个代表修改行数的 tuple。\nInsert 和 Delete 时，记得要更新与 table 相关的所有 index。index 与 table 类似，同样由 Catalog 管理。需要注意的是，由于可以对不同的字段建立 index，一个 table 可能对应多个 index，所有的 index 都需要更新。\nInsert 时，直接将 tuple 追加至 table 尾部。Delete 时，并不是直接删除，而是将 tuple 标记为删除状态，也就是逻辑删除。（在事务提交后，再进行物理删除，Project 3 中无需实现）\nInsert \u0026amp; Delete 的 Next() 只会返回一个包含一个 integer value 的 tuple，表示 table 中有多少行受到了影响。\nIndexScan 使用我们在 Project 2 中实现的 B+Tree Index Iterator，遍历 B+ 树叶子节点。由于我们实现的是非聚簇索引，在叶子节点只能获取到 RID，需要拿着 RID 去 table 查询对应的 tuple。\n在完成 Task 1 的四个算子后，可以用已提供的 sqllogictest 工具和已提供的一些 sql 来检验自己的算子是否实现正确。\n关于 Task 1 的具体实现的确没太多可说的，基本是把官网的 instruction 翻译了一遍。后面几个 task 难度会稍大一点点，也会讲讲更具体的实现。\nTask 2 Aggregation \u0026amp; Join Executors Task 2 包含了 3 个算子，Aggregation、NestedLoopJoin 和 NestedIndexJoin。\nAggregation Aggregation 算子就稍微复杂一点了。先看看 AggregationExecutor 的成员：\n/** The aggregation plan node */ const AggregationPlanNode *plan_; /** The child executor that produces tuples over which the aggregation is computed */ std::unique_ptr\u0026lt;AbstractExecutor\u0026gt; child_; /** Simple aggregation hash table */ SimpleAggregationHashTable aht_; /** Simple aggregation hash table iterator */ SimpleAggregationHashTable::Iterator aht_iterator_; 主要说说这个 SimpleAggregationHashTable。Aggregation 是 pipeline breaker。也就是说，Aggregation 算子会打破 iteration model 的规则。原因是，在 Aggregation 的 Init() 函数中，我们就要将所有结果全部计算出来。原因很简单，比如下面这条 sql：\nSELECT t.x, max(t.y) FROM t GROUP BY t.x; 结果的每条 tuple 都是一个 t.x 的聚合，而要得到同一个 t.x 对应的 max(t.y)，必须要遍历整张表。因此，Aggregation 需要在 Init() 中直接计算出全部结果，将结果暂存，再在 Next() 中一条一条地 emit。而 SimpleAggregationHashTable 就是计算并保存 Aggregation 结果的数据结构。\nSimpleAggregationHashTable 维护一张 hashmap，键为 AggregateKey，值为 AggregateValue，均为 std::vector\u0026lt;Value\u0026gt;。key 代表 group by 的字段的数组，value 则是需要 aggregate 的字段的数组。在下层算子传来一个 tuple 时，将 tuple 的 group by 字段和 aggregate 字段分别提取出来，调用 InsertCombine() 将 group by 和 aggregate 的映射关系存入 SimpleAggregationHashTable。若当前 hashmap 中没有 group by 的记录，则创建初值；若已有记录，则按 aggregate 规则逐一更新所有的 aggregate 字段，例如取 max/min，求 sum 等等。例如下面这条 sql：\nSELECT min(t.z), max(t.z), sum(t.z) FROM t GROUP BY t.x, t.y; group by（AggregateKey）为 {t.x, t.y}，aggregate（AggregateValue）为 {t.z, t.z, t.z}。aggregate 规则为 {min, max, sum}。\n需要额外注意的是 count(column) 和 count(*) 的区别，以及对空值的处理。另外，不需要考虑 hashmap 过大的情况，即整张 hashmap 可以驻留在内存中，不需要通过 Buffer Pool 调用 page 来存储。\n在 Init() 中计算出整张 hashmap 后，在 Next() 中直接利用 hashmap iterator 将结果依次取出。Aggregation 输出的 schema 形式为 group-bys + aggregates。\nNestedLoopJoin Project 3 中只要求实现 NestedLoopJoin，HashJoin 不做强制要求，而是放在了 Leaderboard Optional 里。实际上实现一个 in-memory 的 HashJoin 也不难。Join 应该是经典的数据库性能瓶颈。Andy 在 Lecture 里也详细地量化地对比了各种 Join 的 costs，有兴趣可以看看。\nNestedLoopJoin 算法本身并不难，但比较容易掉进坑里。伪代码大概是这样：\nfor outer_tuple in outer_table: for inner_tuple in inner_table: if inner_tuple matched outer_tuple: emit 有了这个例子，很容易把代码写成：\nwhile (left_child-\u0026gt;Next(\u0026amp;left_tuple)){ while (right_child-\u0026gt;Next(\u0026amp;right_tuple)){ if (left_tuple matches right_tuple){ *tuple = ...; // assemble left \u0026amp; right together return true; } } } return false; 一开始看起来似乎没什么问题。然而很快可以发现有一个严重的错误，right child 在 left child 的第一次循环中就被消耗完了，之后只会返回 false。解决方法很简单，在 Init() 里先把 right child 里的所有 tuple 取出来暂存在一个数组里就好，之后直接访问这个数组。\nwhile (left_child-\u0026gt;Next(\u0026amp;left_tuple)){ for (auto right_tuple : right_tuples){ if (left_tuple matches right_tuple){ *tuple = ...; // assemble left \u0026amp; right together return true; } } } return false; 看起来好像又没什么问题。然而，同一列是可能存在 duplicate value 的。在上层算子每次调用 NestedLoopJoin 的 Next() 时，NestedLoopJoin 都会向下层算子请求新的 left tuple。但有可能上一个 left tuple 还没有和 right child 中所有能匹配的 tuple 匹配完（只匹配了第一个）。\n例如这两张表：\nt1 t2 --------- --------- | x | | x | --------- --------- | 1 | | 1 | | 2 | | 1 | | 3 | | 2 | --------- --------- 现在执行\nSELECT * FROM t1 INNER JOIN t2 ON t1.x = t2.x; t1 中的 1 只会和 t2 的第一个 1 匹配，产生一行输出。再下一次调用 Next() 时，左边会直接选取 2 开始尝试匹配。\n解决方法也很简单，在算子里暂存 left tuple，每次调用 Next() 时，先用暂存的 left tuple 尝试匹配。并且要记录上一次右表匹配到的位置，不要每次都直接从右表第一行开始匹配了。右表遍历完还没有匹配结果，再去找左表要下一个 tuple。\n说来说去，实际上就是注意迭代器要保存上下文信息。\nINNER JOIN 和 LEFT JOIN 按规则实现就好，差不多。LEFT JOIN 注意处理空值。\n还有一个小问题，怎么判断两个 tuple 是否匹配？这里就要第一次遇到 Project 3 里另一个重要的类 AbstractExpression 了。\nAbstractExpression 抽象了 sql 中的各种表达式，包括 ArithmeticExpression、ColumnValueExpression、ComparisonExpression、ConstantValueExpression 和 LogicExpression。这都是什么？看下面这条 sql：\nSELECT * FROM t1 WHERE t1.x = t1.y + 1 AND t1.y \u0026gt; 0; 重点关注 WHERE 后的表达式 t1.x = t1.y + 1 AND t1.y \u0026gt; 0。看这下面这张图：\n其实就是一颗表达式树。AbstractExpression 就是表达式树的节点。sql 中的所有表达式都会被 parse 为表达式树，在 Binder 中进行绑定。上面的 JOIN 中也存在表达式 t1.x = t2.x。AbstractExpression 里最重要的方法就是 Evaluate()，返回值是 value。调用 Evaluate()，参数为 tuple 和 tuple 对应的 schema，返回从这个 tuple 中提取数据后代入表达式计算得到的结果。\n在 NestedLoopJoin 里，我们要用到的是 EvaluateJoin()，也差不多，只不过输入的是左右两个 tuple 和 schema。返回值是表示 true 或 false 的 value。true 则代表成功匹配。\n到这里，NestedLoopJoin 就成功实现了。Join 输出的 schema 为 left schema + right schema。\n后来我看了一下 RisingLight 里的实现，我这个 rustacean 萌新的第一反应是惊为天人。大致是这样：\npub async fn Execute(){ for left_tuple in left_table { for right_tuple in right_table { if matches { yield AssembleOutput(); } } } } 这是一个生成器，当执行到 yield 时，函数会暂时中断，从生成器回到调用者。而调用者再次进入生成器时，可以直接回到上次中断的地方。再配合 stream，就利用 rust 的无栈协程和异步编程完美地实现了一个 NestedLoopJoin 算子，比手动保存上下文信息优雅太多了。\n后来仔细想想，Go 也可以有类似的写法：\nfunc Executor(out_ch, left_ch, right_ch chan Tuple) { // fetch right tuples from right_ch right_tuples := []Tuple{} for right_tuple := range right_ch { right_tuples = append(right_tuples, right_tuple) } for left_tuple := range left_ch { for _, right_tuple := range right_tuples { if matches { out_ch \u0026lt;- AssembleOutput(); } } } close(out_ch) } 每个算子都是一个 goroutine，通过 channel 实现异步的计算，好像也不错。\nNestedIndexJoin 在进行 equi-join 时，如果发现 JOIN ON 右边的字段上建了 index，则 Optimizer 会将 NestedLoopJoin 优化为 NestedIndexJoin。具体实现和 NestedLoopJoin 差不多，只是在尝试匹配右表 tuple 时，会拿 join key 去 B+Tree Index 里进行查询。如果查询到结果，就拿着查到的 RID 去右表获取 tuple 然后装配成结果输出。其他的就不再多说了。\nTask 3 Sort + Limit Executors and Top-N Optimization Task 3 中要实现 3 个算子，Sort、Limit 和 TopN，以及将 Limit + Sort 在 Optimizer 中优化为 TopN。\nSort Sort 也是 pipeline breaker。在 Init() 中读取所有下层算子的 tuple，并按 ORDER BY 的字段升序或降序排序。Sort 算子说起来比较简单，实现也比较简单，主要需要自定义 std::sort()。\nstd::sort() 的第三个参数可以传入自定义的比较函数。直接传入一个 lambda 匿名函数。由于要访问成员 plan_ 来获取排序的字段，lambda 需要捕获 this 指针。另外，排序字段可以有多个，按先后顺序比较。第一个不相等，直接得到结果；相等，则比较第二个。不会出现所有字段全部相等的情况。\nstd::sort(sorted_tuples_.begin(), sorted_tuples_.end(), [this](const Tuple \u0026amp;a, const Tuple \u0026amp;b) { for (auto [order_by_type, expr] : plan_-\u0026gt;GetOrderBy()) { // compare and return ... } UNREACHABLE(\u0026#34;doesn\u0026#39;t support duplicate key\u0026#34;); }); Limit 和 SeqScan 基本一模一样，只不过在内部维护一个 count，记录已经 emit 了多少 tuple。当下层算子空了或 count 达到规定上限后，不再返回新的 tuple。\nTopN 仅需返回最大/最小的 n 个 tuple。一开始想着要实现一个 fixed-size priority queue，即 queue 大小超过限制时自动抛弃最后一个元素以减小内存占用，但后来实在不想自己重写一遍二叉堆，就开摆了。直接用 std::priority_queue 加自定义比较函数，然后在 Init() 中遍历下层算子所有 tuple，全部塞进优先队列后截取前 n 个。再 Next() 里一个一个输出。（是不是和 Limit + Sort 没什么区别？都是 O(nlogn)\nSort + Limit As TopN 这是 Project 3 里最后一个必做的小问。终于不是实现算子了，而是在 Optimizer 里增加一条规则，将 Sort + Limit 优化为 TopN。先看看 Optimizer 是如何执行优化规则的：\nauto Optimizer::OptimizeCustom(const AbstractPlanNodeRef \u0026amp;plan) -\u0026gt; AbstractPlanNodeRef { auto p = plan; p = OptimizeMergeProjection(p); p = OptimizeMergeFilterNLJ(p); p = OptimizeNLJAsIndexJoin(p); p = OptimizeNLJAsHashJoin(p); // Enable this rule after you have implemented hash join. p = OptimizeOrderByAsIndexScan(p); p = OptimizeSortLimitAsTopN(p); // what we should add return p; } 可以看到，让未经优化的原始 plan 树依次经历多条规则，来生成优化过的 plan。我们的任务就是新增一条规则。看看其他规则是怎么实现的，例如 NLJAsIndexJoin：\nauto Optimizer::OptimizeNLJAsIndexJoin(const AbstractPlanNodeRef \u0026amp;plan) -\u0026gt; AbstractPlanNodeRef { std::vector\u0026lt;AbstractPlanNodeRef\u0026gt; children; for (const auto \u0026amp;child : plan-\u0026gt;GetChildren()) { children.emplace_back(OptimizeNLJAsIndexJoin(child)); } auto optimized_plan = plan-\u0026gt;CloneWithChildren(std::move(children)); if (optimized_plan-\u0026gt;GetType() == PlanType::NestedLoopJoin) { // apply the rule and return } return optimized_plan; } 可以看到，实际上就是对 plan tree 进行后序遍历，自底向上地适用规则，改写节点。遍历到某个节点时，通过 if 语句来判断当前节点的类型是否符合我们要优化的类型，若符合则进行优化。\n大致了解如何对 plan 进行优化后，就可以开始写我们的优化规则了。需要特别注意的是，能优化为一个 TopN 算子的形式是，上层节点为 Limit，下层节点为 Sort，不能反过来。同样，我们对 plan tree 进行后续遍历，在遇到 Limit 时，判断其下层节点是否为 Sort，若为 Sort，则将这两个节点替换为一个 TopN。还是比较好实现的，只是代码看起来可能有点复杂。\n到这里，Project 3 中必做的部分就结束了。还剩下选做的 Leaderboard Task。本来也不是 CMU 的学生，就不分什么必做选做了，感兴趣的话都推荐试一试。我个人感觉 Leaderboard Task 还是很好玩的，就是代码写起来有点难受，corner case 比较多。\nLeaderboard Task Leaderboard Task 包含三条极其诡异的 sql，我们要做的就是增加新的优化规则，让这三条 sql 执行地越快越好。分三个部分：\nQuery 1: Where\u0026rsquo;s the Index? Query 2: Too Many Joins! Query 3: The Mad Data Scientist Query 1: Where\u0026rsquo;s the Index? 首先来看一看需要我们优化的 sql：\ncreate index t1x on t1_50k(x); select count(*), max(t1_50k.x), max(t1_50k.y), max(__mock_t2_100k.x), max(__mock_t2_100k.y), max(__mock_t3_1k.x), max(__mock_t3_1k.y) from ( t1_50k inner join __mock_t2_100k on t1_50k.x = __mock_t2_100k.x ) inner join __mock_t3_1k on __mock_t2_100k.y = __mock_t3_1k.y; 稍微把表名替换一下：\ncreate index t1x on t1(x); select count(*), max(t1.x), max(t1.y), max(t2.x), max(t2.y), max(t3.x), max(t3.y) from ( t1 inner join t2 on t1.x = t2.x ) inner join t3 on t2.y = t3.y; 看的我有点精神恍惚，实际上就是三张表 Join 再 Aggregate 一下。主要优化方向是把 NestedLoopJoin 替换为 HashJoin、Join Reorder 让小表驱动大表，以及正确识别 t1.x 上的索引。\n先说 HashJoin。实际上仅需考虑 in-memory 情况时，HashJoin 并不难实现。主要分为两个步骤，Build 和 Probe。Build 阶段在 Init() 进行，遍历左表建立 hashmap。Probe 阶段在 Next() 进行，遍历右表探测是否有 match 的 tuple。需要注意 HashJoin 只能用于优化 equi-join。\n具体实现起来，对于我这种对 modern c++ 极不熟悉的人来说，难点反而在怎么正确构造出这个 hashmap 让编译通过。hashmap 的键应该为 value，但 value 没有重载 operator==，也没有实现自定义 hash 函数，不能直接作为键。\n一开始，我想用 src/include/common/util/hash_util.h 里的 HashValue 函数将 value hash 为 hash_t 类型，然后把 hash_t 作为键。hashmap 直接用 std::unordered_map。但遇到了哈希冲突的问题，还是绕不过要重载 operator==。直接重载 value 的 operator== 是行不通的，autograder 无法识别。因此我定义了 ValueKey 类型把 value 包裹起来，为 ValueKey 重载 == 并实现 hash 函数。这样就可以直接用 ValueKey 作为 hashmap 的键了。至于 operator== 的具体实现，需要关注一下 Value 类的结构，取出 value 的 raw data 并 cast 为正确类型进行比较。同样，hash 函数也是对 raw data 进行 hash。\nhashmap 的值是什么？注意不是 tuple，而是 tuple 数组。同样，因为可能存在 duplicate。\nHashJoin 的实现大致如此，接下来是 Join Reorder。\nJoin Reorder 其实比较简单，可以调用 EstimatedCardinality() 来估计 table 的大小，然后根据大小来调整 plan tree 里连续 join 的顺序即可。\n最后是 Correctly Pick up Index。在原始 NLJAsIndexJoin 里，始终只会尝试为右表匹配 index，左表则被忽略。因此，可以新建一条规则，如果左表有 index，右表没有，且为 equi-join，则把左右顺序替换一下，即有索引的左表换到右边，便于之后正确识别索引。然而我在实现后发现，这是一个负优化（，可能大部分情况下还是 HashJoin 比较靠谱。\nQuery 2: Too Many Joins! 先看看 sql：\nselect count(*), max(__mock_t4_1m.x), max(__mock_t4_1m.y), max(__mock_t5_1m.x), max(__mock_t5_1m.y), max(__mock_t6_1m.x), max(__mock_t6_1m.y) from (select * from __mock_t4_1m, __mock_t5_1m where __mock_t4_1m.x = __mock_t5_1m.x), __mock_t6_1m where (__mock_t6_1m.y = __mock_t5_1m.y) and (__mock_t4_1m.y \u0026gt;= 1000000) and (__mock_t4_1m.y \u0026lt; 1500000) and (__mock_t6_1m.x \u0026lt; 150000) and (__mock_t6_1m.x \u0026gt;= 100000); 更精神恍惚了，简化一下：\nselect count(*), max(t4.x), max(t4.y), max(t5.x), max(t5.y), max(t6.x), max(t6.y) from (select * from t4, t5 where t4.x = t5.x), t6 where (t6.y = t5.y) and (t4.y \u0026gt;= 1000000) and (t4.y \u0026lt; 1500000) and (t6.x \u0026lt; 150000) and (t6.x \u0026gt;= 100000); 这大概是个什么东西呢，大概是所有的 JOIN 全部写成了 FULL JOIN，然后把所有 Filter 放在了 plan tree 的顶端。原始执行计划是这样的：\n需要优化的内容还是比较明显的，Filter Push-down，将 Filter 尽可能地下推至数据源处。需要注意不是所有的 Filter 都可以下推。在本例中，我们只需要把 Filter 正确下推至 Join 算子下就可以了。最终产生的优化方案大致是这样：\n注意要将 Filter 的 predicate 语句正确分类，下推至正确的分支。\n在实现 Filter Push-down 时，一开始我和之前一样，进行后序遍历，自底向上地改写，但是发现这样似乎不能将 Filter 完全地下推，因为一个 Filter 被下推一次后，就无法被再次访问到了，只能被下推一次。因此这次我改用了先序遍历，自顶向下地改写。当下推一个 Filter 后，由于是向下遍历，Filter 还能被再次访问到，可以被继续下推。\n需要注意的时，我们下推的不是整个 Filter 节点，实际上是节点中的 predicate。我的做法是遍历表达式树，提取 predicate 中的所有 comparison，判断表达式的两边是否一个是 column value，一个是 const value，只有这样的 predicate 可以被下推（也存在其他形式的可以下推的 predicate，由于在这里只是对 optimizer 的体验，也只用优化预先给出的 sql，可以稍微简化一下算法，不用考虑太多的 corner case），再将所有的 predicate 重新组合为 logic expression，生成新的 Filter，根据 column value 的 idx 来选择下推的分支。\n两边都为 column value 且分别代表左右两个下层算子的某一列的 Filter 可以被结合到 Join 节点中作为 Join 条件。这一步的规则已经被实现好了，因此这种 Filter 我们让其停留在原地即可。\nQuery 3: The Mad Data Scientist 看看 sql：\nselect v, d1, d2 from ( select v, max(v1) as d1, max(v1) + max(v1) + max(v2) as d2, min(v1), max(v2), min(v2), max(v1) + min(v1), max(v2) + min(v2), min(v1), max(v2), min(v2), max(v1) + min(v1), max(v2) + min(v2), min(v1), max(v2), min(v2), max(v1) + min(v1), max(v2) + min(v2), min(v1), max(v2), min(v2), max(v1) + min(v1), max(v2) + min(v2), min(v1), max(v2), min(v2), max(v1) + min(v1), max(v2) + min(v2), min(v1), max(v2), min(v2), max(v1) + min(v1), max(v2) + min(v2), min(v1), max(v2), min(v2), max(v1) + min(v1), max(v2) + min(v2), min(v1), max(v2), min(v2), max(v1) + min(v1), max(v2) + min(v2) from __mock_t7 left join (select v4 from __mock_t8 where 1 == 2) on v \u0026lt; v4 group by v ) 很怀疑迟先生在写这条 sql 时的精神状态。\n实际上，我们只用 SELECT v, d1, d2，其余的数据都是多余的，无需计算。因此我们需要实现 Column Pruning 优化。\n我实现的 Column Pruning 包括两个部分：\n遇到连续的两个 Projection，合并为 1 个，只取上层 Projection 所需列。 遇到 Projection + Aggregation，改写 aggregates，截取 Projection 中需要的项目，其余直接抛弃。 同样地，我个人认为 Column Pruning 也是自顶向下地改写比较方便。具体实现是收集 Projection 里的所有 column，然后改写下层节点，仅保留上层需要 project 的 column。这里的 column 不一定是表中的 column，而是一种广义的 column，例如 SELECT t1.x + t1.y FROM t1 中的 t1.x + t1.y。\n另外，我们注意到这条 sql 里有一个永为假的 predicate where 1 == 2。对于这种 Filter，我们可以将其优化为 DummyScan，即第一次调用 Next() 就返回 false。可以用一个空的 Value 算子实现。\nSummary 至此，Project 3 就全部完成了。总的来说体验还是很好的，实现了一系列算子，也实现了一系列的优化规则，对查询引擎有了更清晰的认识。\n同样地，有很多实现上具体的细节也忽略掉了，比如如何装配一个中间 tuple，类型系统的设计，table page 的设计等等。这些都与主线关系不大，也就不再唠叨了。\n另外，在测试的过程中，意外发现 Bustub 的 OR 语句无法正确执行，一路找 bug 找上去发现是 Binder 中的一个小 typo，向 Bustub 提了 PR，也被 merge 了。算是为开源课程做了一点微微微小的贡献吧。\n","permalink":"https://blog.eleven.wiki/posts/cmu15-445-project3-query-execution/","summary":"来记录一下 Bustub Query Execution 的实现过程。\n在阅读本文前，墙裂推荐阅读 Project 3 开发者迟先生的这篇文章： BusTub 养成记：从课程项目到 SQL 数据库 可以更清晰地了解到 Bustub SQL 层的设计过程。\nResources https://15445.courses.cs.cmu.edu/fall2022 课程官网 https://github.com/cmu-db/bustub Bustub Github Repo https://www.gradescope.com/ 自动测评网站 GradeScope，course entry code: PXWVR5 https://discord.gg/YF7dMCg Discord 论坛，课程交流用 bilibili 有搬运的课程视频，自寻。 https://15445.courses.cs.cmu.edu/fall2022/bustub/ 在你的浏览器上运行 Bustub！ 请不要将实现代码公开，尊重 Andy 和 TAs 的劳动成果！\nOverview Andy 在 Lecture 中说，Query Optimization 是数据库最难的部分，Transaction 是第二难的部分。总体来说，Project 3 的难度不算大，但和 Project 2 的难点恰好相反：Project 2 的难点在于从零实现 B+ 树，一切都得靠自己。Project 3 的难点在于读代码，理解查询引擎的原理，具体实现起来其实比较简单。\n这是课程官网的一张图，清晰地介绍了 Bustub 的整体架构。在 Project 3 中，我们需要实现一系列 Executors，以及为 Optimizer 添加新功能。","title":"CMU15-445 Project3 Query Execution"},{"content":"来记录一下 Bustub 噩梦 B+ 树的实现过程。\nResources https://15445.courses.cs.cmu.edu/fall2022 课程官网 https://github.com/cmu-db/bustub Bustub Github Repo https://www.gradescope.com/ 自动测评网站 GradeScope，course entry code: PXWVR5 https://discord.gg/YF7dMCg Discord 论坛，课程交流用 bilibili 有搬运的课程视频，自寻。 https://goneill.co.nz/btree-demo.php B+ 树插入删除的动态演示 请不要将实现代码公开，尊重 Andy 和 TAs 的劳动成果！\nOverview Project 2 需要为 Bustub 实现 B+ 树索引。拆分为两个部分：\nCheckpoint1: 单线程 B+ 树 Checkpoint2: 多线程 B+ 树 实验中给出的 B+ 树接口非常简单，基本只有查询、插入和删除三个接口，内部基本没有给出别的辅助函数，可以让我们自由发挥（无从下手）。因此，任何合法的 B+ 树实现都是允许的。\nB+ 树索引在 Bustub 中的位置如图所示：\n需要使用我们在 Project 1 中实现的 buffer pool manager 来获取 page。\nCheckpoint1 Single Thread B+Tree Checkpoint1 分为两个部分：\nTask1: B+Tree pages，B+树中的各种 page。在 Bustub 索引 B+ 树中，所有的节点都是 page。包含 leaf page，internal page ，和它们的父类 tree page。 Task2：B+Tree Data Structure (Insertion, Deletion, Point Search)。Checkpoint1 的重点，即 B+树的插入、删除和单点查询。 Task1 B+Tree Pages Task1 的实现非常简单，都是一些普通的 Getter 和 Setter。这里主要介绍一下 page 的内存布局。\n在 Project 1 中我们第一次与 page 打交道。page 实际上可以存储数据库内很多种类的数据。例如索引和实际的表数据等等。\n/** The actual data that is stored within a page. */ char data_[BUSTUB_PAGE_SIZE]{}; /** The ID of this page. */ page_id_t page_id_ = INVALID_PAGE_ID; /** The pin count of this page. */ int pin_count_ = 0; /** True if the page is dirty, i.e. it is different from its corresponding page on disk. */ bool is_dirty_ = false; /** Page latch. */ ReaderWriterLatch rwlatch_; 其中，data_ 是实际存放 page 数据的地方，大小为 BUSTUB_PAGE_SIZE，为 4KB。其他的成员是 page 的 metadata。\nB+树中的 tree age 数据均存放在 page 的 data 成员中。\nB_PLUS_TREE_PAGE\nb_plus_tree_page 是另外两个 page 的父类，即 B+树中 tree page 的抽象。\nIndexPageType page_type_; // leaf or internal. 4 Byte lsn_t lsn_ // temporarily unused. 4 Byte int size_; // tree page data size(not in byte, in count). 4 Byte int max_size_; // tree page data max size(not in byte, in count). 4 Byte page_id_t parent_page_id_; // 4 Byte page_id_t page_id_; // 4 Byte // 24 Byte in total 以上数据组成了 tree page 的 header。\npage data 的 4KB 中，24Byte 用于存放 header，剩下的则用于存放 tree page 的数据，即 KV 对。\nB_PLUS_TREE_INTERNAL_PAGE\n对应 B+ 树中的内部节点。\nMappingType array_[1]; internal page 中没有新的 metadata，header 大小仍为 24B。它唯一的成员是这个怪怪的大小为 1 的数组。大小为 1 显然不合理，代表只能存放一个 KV 对。但又没法改变它的大小，难道要用 undefined behavior 来越界访问其后的地址？实际上差不多就是这个意思。但这不是 undefined behavior，是一种特殊的写法，叫做 flexible array。我也不知道怎么翻译。\n简单来说就是，当你有一个类，这个类中有一个成员为数组。在用这个类初始化一个对象时，你不能确定该将这个数组的大小设置为多少，但知道这整个对象的大小是多少 byte，你就可以用到 flexible array。flexible array 必须是类中的最后一个成员，并且仅能有一个。在为对象分配内存时，flexible array 会自动填充，占用未被其他变量使用的内存。这样就可以确定自己的长度了。\n例如有一个类 C：\nclass C { int a; // 4 byte int array[1]; // unknown size }; 现在初始化一个 C 的对象，并为其分配了 24 byte 的内存。a 占了 4 byte 内存，那么 array 会尝试填充剩下的内存，大小变为 5。\n实际上这就是 C++ 对象内存布局的一个简单的例子。因此 flexible array 为什么只能有一个且必须放在最后一个就很明显了，因为需要向后尝试填充。\n此外，虽然成员在内存中的先后顺序和声明的顺序一致，但需要注意可能存在的内存对齐的问题。header 中的数据大小都为 4 byte，没有对齐问题。\n到这里，这个大小为 1 的数组的作用就比较清楚了。利用 flexible array 的特性来自动填充 page data 4KB 减掉 header 24byte 后剩余的内存。剩下的这些内存用来存放 KV 对。\ninternal page 中，KV 对的 K 是能够比较大小的索引，V 是 page id，用来指向下一层的节点。Project 中要求，第一个 Key 为空。主要是因为在 internal page 中，n 个 key 可以将数轴划分为 n+1 个区域，也就对应着 n+1 个 value。实际上你也可以把最后一个 key 当作是空的，只要后续的处理自洽就可以了。\n通过比较 key 的大小选中下一层的节点。实际上等号的位置也可以改变，总之，只要是合法的 B+ 树，即节点大小需要满足最大最小值的限制，各种实现细节都是自由的。\n另外需要注意的是，internal page 中的 key 并不代表实际上的索引值，仅仅是作为一个向导，引导需要插入/删除/查询的 key 找到这个 key 真正所在的 leaf page。\nB_PLUS_TREE_LEAF_PAGE\nleaf page 和 internal page 的内存布局基本一样，只是 leaf page 多了一个成员变量 next_page_id，指向下一个 leaf page（用于 range scan）。因此 leaf page 的 header 大小为 28 Byte。\nleaf page 的 KV 对中，K 是实际的索引，V 是 record id。record id 用于识别表中的某一条数据。leaf page 的 KV 对是一一对应的，不像 internal page 的 value 多一个。这里也可以看出来 Bustub 所有的 B+ 树索引，无论是主键索引还是二级索引都是非聚簇索引。\n这里简单介绍一下聚簇索引、非聚簇索引，主键索引、二级索引（非主键索引）的区别。 在聚簇索引里，leaf page 的 value 为表中一条数据的某几个字段或所有字段，一定包含主键字段。而非聚簇索引 leaf page 的 value 是 record id，即指向一条数据的指针。 在使用聚簇索引时，主键索引的 leaf page 包含所有字段，二级索引的 leaf page 包含主键和索引字段。当使用主键查询时，查询到 leaf page 即可获得整条数据。当使用二级索引查询时，若查询字段包含在索引内，可以直接得到结果，但如果查询字段不包含在索引内，则需使用得到的主键字段在主键索引中再次查询，以得到所有的字段，进而得到需要查询的字段，这就是回表的过程。 在使用非聚簇索引时，无论是使用主键查询还是二级索引查询，最终得到的结果都是 record id，需要使用 record id 去查询真正对应的整条记录。 聚簇索引的优点是，整条记录直接存放在 leaf page，无需二次查询，且缓存命中率高，在使用主键查询时性能比较好。缺点则是二级索引可能需要回表，且由于整条数据存放在 leaf page，更新索引的代价很高，页分裂、合并等情况开销比较大。 非聚簇索引的优点是，由于 leaf page 仅存放 record id，更新的代价较低，二级索引的性能和主键索引几乎相同。缺点是查询时均需使用 record id 进行二次查询。\nTask1 的主要内容就是这些。实际上要实现的内容非常简单，重点是理解各个 page 的作用和内存布局。\nTask2 B+Tree Data Structure (Insertion, Deletion, Point Search) Task2 是单线程 B+ 树的重点。首先提供演示一个 B+ 树插入删除操作的 网站。主要是看看 B+ 树插入删除的各种细节变化。当然具体实现是自由的，这仅仅是一个示例。\nSearch\n先从最简单的 Point Search 开始。B+ 树的结构应该都比较熟悉了，节点分为 internal page 和 leaf page，每个 page 上的 key 有序排列。当拿到一个 key 需要查找对应的 value 时，首先需要经由 internal page 递归地向下查找，最终找到 key 所在的 leaf page。这个过程可以简化为一个函数 Findleaf()。\nFindleaf() 从 root page 开始查找。在查找到 leaf page 时直接返回，否则根据 key 在当前 internal page 中找到对应的 child page id，递归调用 Findleaf。根据 key 查找对应 child id 时，由于 key 是有序的，可以直接进行二分搜索。15-445 Lecture 中也介绍了一些其他的方法，比如用 SIMD 并行比较，插值法等等。在这里二分搜索就可以了。\ninternal page 中储存 key 和 child page id，那么在拿到 page id 后如何获得对应的 page 指针？用 Project 1 中实现的 buffer pool。\nPage *page = buffer_pool_manager_-\u0026gt;FetchPage(page_id); 同样地，假如我们需要新建一个 page，也是调用 buffer pool 的 NewPage()。\n在获取到一个 page 后，如何使用这个 page 来存储数据？之前已经提到过，page 的 data_ 字段是实际用于存储数据的 4KB 大小的字节数组。通过 reinterpret_cast 将这个字节数组强制转换为我们要使用的类型，例如 leaf page：\nauto leaf_page = reinterpret_cast\u0026lt;B_PLUS_TREE_LEAF_PAGE_TYPE *\u0026gt;(page-\u0026gt;GetData()) reinterpret_cast 用于无关类型的强制转换，转换方法很简单，原始 bits 不变，只是对这些 bits 用新类型进行了重新的解读。可想而知这种转换非常不安全，需要确保转换后的内存布局仍是合法的。在这里原类型是 byte 数组，新类型是我们需要使用的 tree page。\n找到 leaf page 后，同样是二分查找 key，找到对应的 record id。\n查找的过程比较简单。但还有一个比较重要且复杂的细节，就是 page unpin 的问题。\n我们在拿到 page id 后，调用 buffer pool 的 FetchPage() 函数来获取对应的 page 指针。要注意的是，在使用完 page 之后，需要将 page unpin 掉，否则最终会导致 buffer pool 中的所有 page 都被 pin 住，无法从 disk 读取其他的 page。\n比较合适的做法是，在本次操作中，找出 page 最后一次被使用的地方，并在最后一次使用后 unpin。\nInsert\n与 Search 相同，第一步是根据 key 找到需要插入的 leaf page。同样是调用 Findleaf()。得到 leaf page 后，将 key 插入 leaf page。要注意的是，插入时仍需保证 key 的有序性。同样可以二分搜索找到合适的位置插入。\n在插入后，需要检查当前 leaf page size 是否等于 max size。若相等，则要进行一次 leaf page 分裂操作。具体步骤为：\n新建一个空的 page， 将原 page 的一半转移到新 page 中，（假如选择将新 page 放在原 page 右侧，则转移原 page 的右半部分） 更新原 page 和新 page 的 next page id， 获取 parent page， 将用于区分原 page 和新 page 的 key 插入 parent page 中， 更新 parent page 所有 child page 的父节点指针。 这些步骤都比较好理解。需要给 parent page 插入一个新 key 的原因是，多了一个子节点，自然需要多一个 key 来区分。其中第 4 步是重点。获取 parent page 并不是简单地通过当前 page 的 parent id 来获取，因为 parent page 也可能发生分裂。\n假如我们有一棵 5 阶的 B+ 树。5 阶只是一种常用的说法，代表 B+ 树节点最多能容纳五个 KV 对。对于 leaf page 来说，当 B+ 树处于稳定状态时（插入、删除等操作已经完全结束），最多只能有 4 个 KV 对。对于 internal page，最多有 4 个 key，5 个 value，可以看成是有 5 个 KV 对。\n因此，instruction 中有这么一句话：\nYou should correctly perform splits if insertion triggers the splitting condition (number of key/value pairs AFTER insertion equals to max_size for leaf nodes, number of children BEFORE insertion equals to max_size for internal nodes.).\n在插入后检测 leaf page 是否需要分裂，因为 leaf page 稳定时只有 4 个 KV 对，插入后有 5 个，仍能容纳。在插入前检测 internal page，因为 internal page 稳定时就有 5 个 KV 对，若插入后再检测，插入的第 6 个 KV 对会造成越界。\n实际上这也只是一种约定，并不是强制的规则。\n在第 5 步我们可以拿到 parent page 安全地插入 KV 对，是因为在第 4 步中，我们需要返回一个安全的 parent page。\n第 4 步具体操作如下：\n根据 parent page id 拿到 parent page， 判断 parent page size 是否等于 max size，（插入前检查） 若小于，直接返回 parent page， 否则，分裂当前 internal page。并根据此后需要插入的 key 选择分裂后的两个 page 之一作为 parent page 返回。 分裂 internal page 的步骤为：\n新建一个空的 page， 将原 page 的一半转移到新 page 中，需要注意原 page 和新 page 的第一个 key 都是无效的， 更新新 page 所有 child page 的父节点指针，指向新 page， 获取 parent page， 将用于区分原 page 和新 page 的 key 插入 parent page 中， 更新 parent page 所有 child page 的父节点指针。 可以发现，第 4 步同样是需要重复上述步骤。这里就发生了向上的递归，直到遇到安全的父节点或者遇到根节点。在遇到根节点时，若根节点也需要分裂，则除了需要新建一个节点用来容纳原根节点一半的 KV 对，还需要新建一个新的根节点。\n另外需要注意一个细节，在 leaf page 分裂时，向父节点插入 key 时是复制后插入，而 internal page 分裂时，向父节点插入 key 是删除后插入，有点像把 key 上推。\n假如有一棵 4 阶的 B+ 树：\n这些细节都可以在 B+ 树示例网站上先尝试一遍，再来实现会简单很多。\nInsert 的整个流程大致就是先向下递归找到 leaf page，插入 KV 后再向上递归分裂。\nDelete\n同样地，先找到 leaf page。删除 leaf page 中 key 对应的 KV 对后，检查 size 是否小于 min size。如果小于的话，首先尝试从两侧的兄弟节点中偷一个 KV 对。注意只能从兄弟节点，即父节点相同的节点中选取。假如存在一侧节点有富余的 KV 对，则成功偷取，结束操作。若两侧都没有富余的 KV 对，则选择一侧节点与其合并。\n偷取的过程比较简单，从左侧节点偷取时，把左侧节点最后一个 KV 对转移至当前节点第一个 KV 对，从右侧节点偷取时，把右侧节点的 KV 对转移至当前节点最后一个 KV 对。leaf page 和 internal page 的偷取过程基本相同，仅需注意 internal page 偷取后更新子节点的父节点指针。\n稍难的是合并的过程。同样，任选左右侧一兄弟节点进行合并。将一个节点的所有 KV 对转移至另一节点。若合并的是 leaf page，记得更新 next page id。若合并的是 internal page，记得更新合并后 page 的子节点的父节点指针。然后，删除 parent 节点中对应的 key。删除后，再次检查 size 是否小于 min size，形成向上递归。\n当合并 leaf page 后，删除父节点中对应的 key 比较简单，直接删除即可。例如 4 阶 B+ 树：\n合并 internal page 后，并不是简单地删除父节点中对应 key，而是有一个父节点 key 下推的过程：\n需要注意的是，root page 并不受 min size 的限制。但如果 root page 被删到 size 只剩 1，即只有一个 child page 的时候，应将此 child page 设置为新的 root page。\n另外，在合并时，两个 page 合并成一个 page，另一个 page 应该删除，释放资源。删除 page 时，仍是调用 buffer pool 的 DeletePage() 函数。\n和 Insert 类似，Delete 过程也是先向下递归查询 leaf page，不满足 min size 后先尝试偷取，无法偷取则合并，并向上递归地检查是否满足 min size。\nDebug Your B+Tree 再次感叹真正的世界一流 CS 高校对课程项目设计的用心与体贴。为了方便调试，15-445 竟然帮我们实现了 B+ 树的可视化。有两种主要的方式：\n使用已实现好的 b_plus_tree_printer 工具，可以自己对 B+ 树执行插入、删除等操作，并将结果输出为 dot 文件。 $ # To build the tool $ mkdir build $ cd build $ make b_plus_tree_printer -j$(nproc) $ ./bin/b_plus_tree_printer \u0026gt;\u0026gt; ... USAGE ... \u0026gt;\u0026gt; 5 5 # set leaf node and internal node max size to be 5 \u0026gt;\u0026gt; f input.txt # Insert into the tree with some inserts \u0026gt;\u0026gt; g my-tree.dot # output the tree to dot format \u0026gt;\u0026gt; q # Quit the test (Or use another terminal) 在代码中调用 BPlusTree 的 Draw() 函数，可以在指定目录生成一个 dot 文件。 拿到 dot 文件后，可以在本地生成对应的 B+ 树 png：\ndot -Tpng -O my-tree.dot 或者把文件内容复制到 这里。（更推荐，生成 svg，对 B+ 树大小无限制）\n这个可视化工具对早期发现 B+ 树的各种基本 bug 非常有用。\n至此，Checkpoint1 的内容已经全部完成。其实有很多细节我都没有提到，比如二分搜索的边界问题，如何查询左右兄弟节点，如何在兄弟节点间移动 KV 对，第一次 Insert 树为空怎么办等等。这些都属于比较细枝末节的问题，比较折磨人，但认真思考应该都能够解决。更重要的是，这些实现都是自由的。\nCheckpoint2 Multi Thread B+Tree Checkpoint2 也分为两个部分：\nTask3：Index Iterator。实现 leaf page 的 range scan。 Task4：Concurrent Index。支持 B+ 树并发操作。 Task3 Index Iterator 这个部分没有什么太多好说的，实现一个遍历 leaf page 的迭代器。在迭代器中存储当前 leaf page 的指针和当前停留的位置即可。遍历完当前 page 后，通过 next page id 找到下一个 leaf page。同样，记得 unpin 已经遍历完的 page。关于可能存在的死锁问题，暂时不讨论。\nTask4 Concurrent Index 这是并发 B+ 树的重点，应该也是 Project2 中最难的部分。我们要使此前实现的 B+ 树支持并发的 Search/Insert/Delete 操作。整棵树一把锁逻辑上来说当然是可以的，但性能也会可想而知地糟糕。在这里，我们会使用一种特殊的加锁方式，叫做 latch crabbing。顾名思义，就像螃蟹一样，移动一只脚，放下，移动另一只脚，再放下。基本思想是：\n先锁住 parent page， 再锁住 child page， 假设 child page 是安全的，则释放 parent page 的锁。安全指当前 page 在当前操作下一定不会发生 split/steal/merge。同时，安全对不同操作的定义是不同的，Search 时，任何节点都安全；Insert 时，判断 max size；Delete 时，判断 min size。 这么做的原因和正确性还是比较明显的。当 page 为安全的时候，当前操作仅可能改变此 page 及其 child page 的值，因此可以提前释放掉其祖先的锁来提高并发性能。\nSearch\nSearch 时，从 root page 开始，先给 parent 上读锁，再给 child page 上读锁，然后释放 parent page 的锁。如此向下递归。\nInsert\nInsert 时，从 root page 开始，先给 parent 上写锁，再给 child page 上写锁。假如 child page 安全，则释放所有祖先的锁；否则不释放锁，继续向下递归。\n在 child page 不安全时，需要持续持有祖先的写锁。并在出现安全的 child page 后，释放所有祖先写锁。如何记录哪些 page 当前持有锁？这里就要用到在 Checkpoint1 里一直没有提到的一个参数，transaction。\ntransaction 就是 Bustub 里的事务。在 Project2 中，可以暂时不用理解事务是什么，而是将其看作当前在对 B+ 树进行操作的线程。调用 transaction 的 AddIntoPageSet() 方法，来跟踪当前线程获取的 page 锁。在发现一个安全的 child page 后，将 transaction 中记录的 page 锁全部释放掉。按理来说，释放锁的顺序可以从上到下也可以从下到上，但由于上层节点的竞争一般更加激烈，所以最好是从上到下地释放锁。\n在完成整个 Insert 操作后，释放所有锁。\nDelete\n和 Insert 基本一样。仅是判断是否安全的方法不同（检测 min size）。需要另外注意的是，当需要 steal/merge sibling 时，也需要对 sibling 加锁。并在完成 steal/merge 后马上释放。这里是为了避免其他线程正在对 sibling 进行 Search/Insert 操作，从而发生 data race。这里的加锁就不需要在 transaction 里记录了，只是临时使用。\nImplementation\n可以发现，latch crabbing 是在 Find Leaf 的过程中进行的，因此需要修改 Checkpoint1 中的 FindLeaf()，根据操作的不同沿途加锁。\nWhen should we unlatch and unpin pages? 在这里，我还想提一提 unpin page 的问题。在前面我只是简单地说了一句在最后一次使用的地方 unpin。但实际上，这个问题在整个 Project2 中时时困扰着我。特别是在 Checkpoint2 中引入 page 锁之后。到底该如何优雅地释放我们获得的 page？\n首先，为什么要 unpin page？这个应该比较清楚了，避免对 buffer pool 一直占用。可以理解为一种资源的泄露。\n说到资源泄露，可以自然地想到 RAII。RAII 的主要思想是，在初始化时获取资源，在析构时释放资源。这样就避免了程序中途退出，或抛出异常后，资源没有被成功释放。常常用在 open socket、acquire mutex 等操作中。其实我们在 Project1 中已经遇到了 RAII 的用法：\nstd::scoped_lock\u0026lt;std::mutex\u0026gt; lock(mutex_); 这里其实就是一个经典的 RAII。在初始化 lock 时，调用 mutex_.Lock()，在析构 lock 时，调用 mutex_.Unlock()。这样就通过简单的一行代码成功保证了在 lock 的作用域中对 mutex 全程上锁。离开 lock 的作用域后，由于 lock 析构，锁自动释放。\n一开始，我也想过用这种方法来管理 page。例如编写一个类 PageManager，在初始化时，fetch page \u0026amp; latch page，在析构时，unlatch page \u0026amp; unpin page。这个想法好像还行，但是遇到了一个明显的问题：page 会在不同函数间互相传递，以及存在离开作用域后仍需持有 page 资源的情况，比如 latch crabbing 时可能需要跨函数持锁。或许可以通过传递 PageManager 指针的方式来处理，但这样似乎更加复杂了。\n此外，还有一个问题。比如 Insert 操作时，假如需要分裂，会向下递归沿途持锁，然后向上递归进行分裂。在分裂时，需要重新从 buffer pool 获取 page。要注意的是，这里获取 page 时不能够对 page 加锁，因为此前向下递归时 page 已经加过锁了，同一个线程再加锁会抛异常。\n比如这里的例子。在向上递归时，我们已经获取过 parent page 的锁，因此再次从 buffer pool 获取 parent page 时，无需对 parent page 再次加锁。\n那有没有办法能够知道我们对哪些 page 加过锁？transaction。也就是说，如果一个 page 出现在 transaction 的 page set 中，就代表这个线程已经持有了这个 page 的锁。\n当然，通过认真分析各个操作获取 page 的路径，我们也可以发现持锁的规律。\nSearch\n仅向下递归，拿到 child page 就释放 parent page。这个比较简单。获取 page 的路径从 root 到 leaf 是一条线。到达 leaf 时，仅持有 leaf 的资源。\nInsert\n先向下递归，可能会持有多个 parent page 的锁。获取 page 的路径从 root 到 leaf 也是一条线，区别是，到达 leaf 时，还可能持有其祖先的资源。再向上递归。向上递归的路径与向下递归的完全重合，仅是方向相反。因此，向上递归时不需要重复获取 page 资源，可以直接从 transaction 里拿到 page 指针，绕过对 buffer pool 的访问。在分裂时，新建的 page 由于还未连接到树中，不可能被其他线程访问到，因此也不需要上锁，仅需 unpin。\nDelete\n向下递归的情况与 Insert 相同，路径为一条线。到达 leaf page 后，情况有所不同。由于可能需要对 sibling 进行 steal/merge，还需获取 sibling 的资源。因此，在向上递归时，主要路径也与向下递归的重合，但除了这条线，还会沿途获取 sibling 的资源，sibling 需要加锁，而 parent page 无需再次加锁。sibling 只是暂时使用，使用完之后可以直接释放。而向下递归路径上的锁在整个 Delete 操作完成之后再释放。\n经过上面的讨论，可以得出我们释放资源的时机：向下递归路径上的 page 需要全程持有（除非节点安全，提前释放），在整个操作完成后统一释放。其余 page 要么是重复获取，要么是暂时获取。重复获取无需加锁，使用完后直接 unpin。暂时获取（steal/merge sibling）需要加锁，使用完后 unlatch \u0026amp; unpin。\nDeadlock？ 可以看出，需要持多个锁时，都是从上到下地获取锁，获取锁的方向是相同的。在对 sibling 上锁时，一定持有其 parent page 的锁，因此不可能存在另一个既持有 sibling 锁又持有 parent page 锁的线程来造成循环等待。因此，死锁是不存在的。\n但如果把 Index Iterator 也纳入讨论，就有可能产生死锁了。Index Iterator 是从左到右地获取 leaf page 的锁，假如存在一个需要 steal/merge 的 page 尝试获取其 left sibling 的锁，则一个从左到右，一个从右到左，可能会造成循环等待，也就是死锁。因此在 Index Iterator 无法获取锁时，应放弃获取。\nOptimization 对于 latch crabbing，存在一种比较简单的优化。在普通的 latch crabbing 中，Insert/Delete 均需对节点上写锁，而越上层的节点被访问的可能性越大，锁竞争也越激烈，频繁对上层节点上互斥的写锁对性能影响较大。因此可以做出如下优化：\nSearch 操作不变，在 Insert/Delete 操作中，我们可以先乐观地认为不会发生 split/steal/merge，对沿途的节点上读锁，并及时释放，对 leaf page 上写锁。当发现操作对 leaf page 确实不会造成 split/steal/merge 时，可以直接完成操作。当发现操作会使 leaf page split/steal/merge 时，则放弃所有持有的锁，从 root page 开始重新悲观地进行这次操作，即沿途上写锁。\n这个优化实现起来比较简单，修改一下 FindLeaf() 即可。\nSummary 整个 Project2 的内容大致就是这些。难度相对于 Project1 可以说是陡增。Checkpoint1 的难点主要在细节的处理上，Checkpoint2 的难点则是对 latch crabbing 的正确理解。当看到自己从 0 实现的 B+ 树能够正确运行，特别是可视化时，还是很有成就感的。\n","permalink":"https://blog.eleven.wiki/posts/cmu15-445-project2-b+tree-index/","summary":"来记录一下 Bustub 噩梦 B+ 树的实现过程。\nResources https://15445.courses.cs.cmu.edu/fall2022 课程官网 https://github.com/cmu-db/bustub Bustub Github Repo https://www.gradescope.com/ 自动测评网站 GradeScope，course entry code: PXWVR5 https://discord.gg/YF7dMCg Discord 论坛，课程交流用 bilibili 有搬运的课程视频，自寻。 https://goneill.co.nz/btree-demo.php B+ 树插入删除的动态演示 请不要将实现代码公开，尊重 Andy 和 TAs 的劳动成果！\nOverview Project 2 需要为 Bustub 实现 B+ 树索引。拆分为两个部分：\nCheckpoint1: 单线程 B+ 树 Checkpoint2: 多线程 B+ 树 实验中给出的 B+ 树接口非常简单，基本只有查询、插入和删除三个接口，内部基本没有给出别的辅助函数，可以让我们自由发挥（无从下手）。因此，任何合法的 B+ 树实现都是允许的。\nB+ 树索引在 Bustub 中的位置如图所示：\n需要使用我们在 Project 1 中实现的 buffer pool manager 来获取 page。\nCheckpoint1 Single Thread B+Tree Checkpoint1 分为两个部分：","title":"CMU15-445 Project2 B+Tree Index"},{"content":"完成 15-445 Project 1 后，将代码提交到 GradeScope，运行时长为 8s 左右。而 Leaderboard 中排名第一的大佬耗时仅为 1s，于是尝试优化 Project 1 的代码来刷刷 Leaderboard 排名，至少差距不要太大\u0026hellip;\n之前没有 C++ 性能优化的经验，一通搜索找到了 perf。perf 是 linux 下的一款性能调优工具。\nInstallation 安装方式 (Ubuntu 20.04 LTS)：\nsudo apt-get install linux-tools-$(uname -r) linux-tools-generic -y Commands sudo perf record -g -a /path/to/your/application 运行一个程序，并对其采样，生成相关数据 perf.data。-g 表明记录函数调用关系，-a 为路径名，还有一个可选参数 -e，表明需要监控的事件。\nperf 有一个监控事件列表，包含 cpu clock，cache miss，page fault 等各种事件。可以用\nsudo perf list 查看。默认为 cpu clock。\n生成 perf.data 后，执行\nsudo perf report -g 查看报告。\n在报告中可以看到各个函数的使用情况以及调用关系。\n再查找热点函数，看看耗时较高的原因是什么，进行相关的优化就可以了。\nperf 还有生成火焰图等更高阶的功能，以后也可以尝试一下。\nOptimization 我主要对之前的代码进行了两个地方的优化。\n一是 LRU-K Replacer 中，我原本用一个 std::unordered_map 存储 frames 的信息，在报告中发现性能并不好，因为有频繁的哈希表插入删除操作。之后改为了直接用 std::vector 存放。删除时，将相应的 frame 置为空指针。\n第二个也是 LRU-K Replacer 中的问题。在 LRU-K 算法中，如果存在引用次数小于 K 的 frame，则要在这些 frame 中使用普通的 LRU 算法，即找到第一次引用时间最小的 frame。在查找 frame 的第一次引用时间时，我直接简单地遍历引用记录。由于使用 std::list 存放引用记录，而链表的遍历是 O(n)，因此这里也出现了一定的性能问题。因此我额外用了一个 earliest_ref 变量来保存第一次引用的时间。只要在第一次发生引用时初始化这个变量，之后每次查询第一次引用时直接返回这个变量。\n经过优化后，再次使用 perf 生成报告，可以发现相关的函数耗时下降不少。\nResult 再次在 GradeScope 上提交，总耗时成功降到了 3.5s 左右，优化效果还是挺明显的。虽说还有挺大的优化空间，但目前的性能应该也够用了。\n还是没有进行针对多线程的性能优化，下次一定。\n","permalink":"https://blog.eleven.wiki/posts/cmu15-445-branch-use-perf-to-benchmark-your-code/","summary":"完成 15-445 Project 1 后，将代码提交到 GradeScope，运行时长为 8s 左右。而 Leaderboard 中排名第一的大佬耗时仅为 1s，于是尝试优化 Project 1 的代码来刷刷 Leaderboard 排名，至少差距不要太大\u0026hellip;\n之前没有 C++ 性能优化的经验，一通搜索找到了 perf。perf 是 linux 下的一款性能调优工具。\nInstallation 安装方式 (Ubuntu 20.04 LTS)：\nsudo apt-get install linux-tools-$(uname -r) linux-tools-generic -y Commands sudo perf record -g -a /path/to/your/application 运行一个程序，并对其采样，生成相关数据 perf.data。-g 表明记录函数调用关系，-a 为路径名，还有一个可选参数 -e，表明需要监控的事件。\nperf 有一个监控事件列表，包含 cpu clock，cache miss，page fault 等各种事件。可以用\nsudo perf list 查看。默认为 cpu clock。\n生成 perf.data 后，执行\nsudo perf report -g 查看报告。\n在报告中可以看到各个函数的使用情况以及调用关系。\n再查找热点函数，看看耗时较高的原因是什么，进行相关的优化就可以了。","title":"CMU15-445 Branch: Use Perf to Benchmark Your Code"},{"content":"暂时战略放弃了 6.824 的 Lab4，来做做 CMU 新鲜出炉的 15-445 FALL 2022。\nResources https://15445.courses.cs.cmu.edu/fall2022 课程官网 https://github.com/cmu-db/bustub Bustub Github Repo https://www.gradescope.com/ 自动测评网站 GradeScope，course entry code: PXWVR5 https://discord.gg/YF7dMCg Discord 论坛，课程交流用 bilibili 有搬运的课程视频，自寻。 请不要将实现代码公开，尊重 Andy 和 TAs 的劳动成果！\nSet up Environment 实验需要 Linux 环境。虽说 Docker 什么的似乎也可以，但 Linux 总是更令人安心。为了方便测试，我选择了云服务器。\nJetBrains 家推出了新的 SSH 远程开发功能。本来想试试，结果 CLion 在 server 上足足吃了两三个 G 的内存，我 2 核 4G 的服务器不堪重负，还是老老实实用 vscode remote。\nDebug 推荐用 lldb，在 vscode 安装相关插件后体验很好。终于不用整天翻看 6.824 又臭又长的 log 了。\nOverview Project1 主要与 Bustub 的 storage manager 相关，分为三个部分：\nExtendible Hash Table LRU-K Replacer Buffer Pool Manager Instance 其中 Extendible Hash Table 和 LRU-K Replacer 是 Buffer Pool Manager 内部的组件，而 Buffer Pool Manager 则是向系统提供了获取 page 的接口。系统拿着一个 page_id 就可以向 Buffer Pool Manager 索要对应的 page，而不关心这个 page 具体存放在哪。系统并不关心（也不可见）获取这个 page 的过程，无论是从 disk 还是 memory 上读取，还是 page 可能发生的在 disk 和 memory 间的移动。这些内部的操作交由 Buffer Pool Manager 完成。\nDisk Manager 已经为我们提供，是实际在 disk 上读写数据的接口。\nExtendible Hash Table Extendible Hash Table Design 这个部分要实现一个 extendible 哈希表，内部不可以用 built-in 的哈希表，比如 unordered_map。这个哈希表在 Buffer Pool Manager 中主要用来存储 buffer pool 中 page id 和 frame id 的映射关系。\nExtendible Hash Table 由一个 directory 和多个 bucket 组成。\ndirectory: 存放指向 bucket 的指针，是一个数组。用于寻找 key 对应 value 所在的 bucket。 bucket: 存放 value，是一个链表。一个 bucket 可以至多存放指定数量的 value。 Extendible Hash Table 与 Chained Hash Table 最大的区别是，Extendible Hash 中，不同的指针可以指向同一个 bucket，而 Chained Hash 中每个指针对应一个 bucket。\n发生冲突时，Chained Hash 简单地将新的 value 追加到其 key 对应 bucket 链表的最后，也就是说 Chained Hash 的 bucket 没有容量上限。而 Extendible Hash 中，如果 bucket 到达容量上限，则对桶会进行一次 split 操作。\n在介绍 split 之前，我们先介绍一下 Extendible Hash 的插入流程。\n将一个键值对 (K,V) 插入哈希表时，会先用哈希函数计算 K 的哈希值 H(K)，并用此哈希值计算出索引，将 V 放入索引对应的 bucket 中。\nExtendible Hash 计算索引的方式是直接取哈希值 H(K) 的低 n 位。在这里，我们把 n 叫做 global depth。例如，K 对应的 H(K) = 1010 0010b，此时 global depth 为 4，则对应的 index 为 0010，即应将 V 放入 directory 里 index 为 2 的指针指向的 bucket 中。\nglobal depth 的初始值为 0，取 H(K) 的低 0 位为索引，永远为 0，即初始时只有一个 bucket。\n我们指定 bucket 的容量为 2，现在向表中插入 KV 对。方便起见，就不具体指明 V 的值了，仅关注 K。假设 H(K) = K。\n首先插入 0 和 1。由于 global depth 为 0，所以 H(K) 计算出的 index 均为 0：\n再插入 2。index 仍然为 0。而此时 bucket 已满，无法继续插入 2，则需要进行之前提到的 split 操作。这时的 split 包含如下几个步骤：\nglobal depth++ directory 容量翻倍 创建一个新的 bucket 重新安排指针 重新分配 KV 对 流程如下：\n到目前为止应该还是比较容易理解的。global depth++ 后为 1，需要取 H(K) 的低 1 位作为 index，index 就有了 0 1 之分。因此 dir 拥有的指针数需要翻倍。仅仅是 index 数量翻倍还不够，此时 0 和 1 仍然指向同一个 bucket，仍然没有空间插入新值，因此还需要新创建一个 bucket。创建 bucket 后，自然需要将 dir 指针重新安排，0 指向 bucket 0，1 指向 bucket 1。\n为什么 KV 对也需要重新分配？假设不重新分配 KV 对，现在有一个 find 请求，查找 K=1 对应的 V。H(K)=1，global depth=1，则 index=1。而此时 index=1 对应的 bucket 空空如也。\n因此为了保证原数据与新的表结构兼容，需要重新计算发生 split 的 bucket，即 bucket 0 中所有 KV 对的新位置，并重新分配。\n现在，我们就有了合适的位置来插入 2：\n接下来我们尝试插入 4。global depth 为 1，H(K)= 100b，index=0。而 index=0 指向的 bucket 又满了，再对 bucket 0 进行一次 split：\n到这里就体现了 Extendible Hash 特殊之处：多个 index 可以指向同一个 bucket。为了支持这种特性，要引入一个新的变量，local depth。\n每个 bucket 都有一个自己的 local depth。bucket 实际上只用到了 H(K) 的低 local depth 位作为索引。local depth 的初始值为 0。在 bucket 发生 split 时，local depth++：\nbucket 0 和 bucket 2 的 local depth 均为 2，即他们实际上都用到了 H(K) 的低 2 位作为 index。例如，H(0)=00b，H(2)=10b，则 0 和 2 对应的 index 分别为 0 和 2，实际上也被分配在了 bucket 0 和 2。\nbucket 1 的 local depth 为 1，其中存放的值实际上只用到了 H(K) 的低 1 位。例如 H(1)=01b，H(3)=11b，index 分别为 1 和 3，但实际上只用到了低 1 位，均为 1，因此 1 和 3 均被放在 bucket 1 中。\n经过第 2 次 split，我们有了放入 4 的空间。H(4)=100b，global depth=2，index=0，即 4 被放入 dir[0] 对应的 bucket 0 中。\n插入 3，成功插入到 bucket 1 中。\n再插入 5。此时 bucket 1 已满，需要对 bucket 1 进行 split。但我们可以发现，这次插入并不需要将 dir 的容量翻倍，仅需新建一个 bucket 3，将 index 3 对应指针指向 bucket 3，并将原 bucket 1 中的 KV 对重新分配到 bucket 1 和 bucket 3 中：\n也就是说，当 bucket 需要分裂时，如果此时已经有多个指针指向 bucket，无需对 dir 进行扩容，仅执行原 5 步中的第 3、4、5 步。在代码中的体现就是，当需要插入 K 时，发现 bucket 已满，首先判断当前 bucket 的 local depth 是否等于 global depth：\n若相等，即仅有一个指针指向 bucket，需要对 dir 扩容。 若不相等，即有多个指针指向 bucket，则无需扩容，将原来指向此 bucket 的指针重新分配。 现在还剩下几个关键的问题：\ndir 扩容时，新的指针应该指向哪里？ 假如 global depth=2，原索引为 000b 001b 010b 011b，则扩容添加的索引为 100b 101b 110b 111b，可以看出低两位的值是一一对应的，新索引应指向低位对应索引的 bucket。我们把指向同一个 bucket 的指针称为兄弟指针。\n如何重新安排指针？ 重新安排指针实际上是重新安排指向需要 split 的 bucket 的兄弟指针。需要注意的是，兄弟指针不一定只有两个，而可以有 2^n 次个。例如下面这种情况：\n当我们插入 0 1 3 5 9 时，就会出现这种情况，其中 bucket 0 共有 2^2 个兄弟指针。实际上，兄弟指针的个数为 2^(global depth - local depth)。那么得知一个 index 后，如何找到这个 index 所有兄弟指针？仍然以上面为例。插入 2，插入至 bucket 0，再插入 4，bucket 0 已满，进行一次 split。H(4) = 100b，对应 index 是 4。因此需要找到 index 4 的所有兄弟指针。bucket 0 的 local depth 为 1，即只用到了低 1 位。100b 的低 1 位为 0。那么其兄弟指针的低 1 位也应是 0，即 000b 010b 100b 110b，分别为 0 2 4 6。这样我们就找到了所有的兄弟指针。接下来将兄弟指针重新分配。local depth 变为 2，用到低 2 位，则兄弟指针可以分为两组，x00b 和 x10b，即 0 4 一组，2 6 一组。其中，一组指向原 bucket 0，另一组指向新 bucket 4。这样就完成了指针的重新分配。 其他情况也是类似的，先通过低位相同的特征找到所有兄弟指针，再将兄弟指针按照新位是 0 还是 1 分为两组，分别指向原 bucket 和新 bucket。\n如何重新分配 KV 对？ 仅需用 global depth 重新计算一遍 K 对应的 index 并插入对应 bucket。\n到这里 Extendible Hash Table 就介绍完毕了。接下来说几个实现上的小细节。\nExtendible Hash Table Implementation Extendible Hash Table 是要保证线程安全的。目前我的策略是一把大锁报平安。这样做多线程的性能肯定是很糟的。实际上应该是整个 table 一把大锁，再分区加多把小锁，或者更简单的做法，每个 bucket 一把小锁。均使用读写锁。\n对于每个 bucket，在 Find 时上读锁，Insert 和 Remove 时上写锁。\n对于整张表，Find 和 Remove 时上读锁。Find 上读锁好理解，而 Remove 实际上只会改变 bucket 的内部变量，其线程安全由 bucket 内部锁保证，因此也可以只上读锁。Insert 在无需 split 时也可以仅上读锁，需要 split 时上写锁。\n我按照这个思路尝试优化了一下，结果成功负优化。可能是哪里出了点问题，有空再回头看看。先一把大锁凑合用。\n另一个小细节是，每次 Insert 前要判断一下是否需要 split。而 split 之后不一定代表可以直接 Insert，因为可能重新分配 KV 对时，所有的 KV 对又被塞到了同一个 bucket 里，而凑巧的是需要插入的 KV 对也被带到了这个 bucket。因此需要循环判断，可能需要多次 split 才能成功插入。\nLRU-K Replacer LRU-K Replacer Design LRU-K Replacer 用于存储 buffer pool 中 page 被引用的记录，并根据引用记录来选出在 buffer pool 满时需要被驱逐的 page。\nLRU 应该都比较熟悉了，LRU-K 则是一个小小的变种。\n在普通的 LRU 中，我们仅需记录 page 最近一次被引用的时间，在驱逐时，选择最近一次引用时间最早的 page。\n在 LRU-K 中，我们需要记录 page 最近 K 次被引用的时间。假如 list 中所有 page 都被引用了大于等于 K 次，则比较最近第 K 次被引用的时间，驱逐最早的。假如 list 中存在引用次数少于 K 次的 page，则将这些 page 挑选出来，用普通的 LRU 来比较这些 page 第一次被引用的时间，驱逐最早的。\n另外还需要注意一点，LRU-K Replacer 中的 page 有一个 evictable 属性，当一个 page 的 evicitable 为 false 时，上述算法跳过此 page 即可。这里主要是为了上层调用者可以 pin 住一个 page，对其进行一些读写操作，此时需要保证 page 驻留在内存中。\nLRU-K 的算法还是比较简单的，主要看具体实现部分。\nLRU-K Replacer Implementation 先是线程安全，LRU-K Replacer 的线程安全似乎并没有什么可以优化的地方，直接加一把大锁就可以了。\n传统的 LRU 用哈希表加双向链表实现，可以保证各操作均为 O(1) 的复杂度。由于 LRU-K 需要保存 K 次引用的记录，就不能再用双向链表了。\n先介绍一下 frame 的概念。page 放置在 buffer pool 的 frame 中，frame 的数量是固定的，即 buffer pool 的大小。如果 buffer pool 中的一个 frame 没有用来放置任何 page，则说该 frame 是空闲的。如果所有 frame 都不是空闲的，则表明 buffer pool 已满。\nReplacer 里有一张哈希表，用于存储不同 frame 的信息。将 frame 的信息封装成 FrameInfo 类。哈希表的 key 为 frame id，value 为 FrameInfo。\nFrameInfo 里用链表保存 page 最近 K 次被引用的时间戳。这里的时间戳不适合也不需要用真正的 Unix 时间戳，直接用一个从 0 递增的 size_t 变量 curr_timestamp 来表示即可，每次引用 Replacer 中的任一 page 时，记录该次引用的时间戳为 curr_timestamp，并将其加 1。size_t 的大小也保证了这个变量不会溢出。\n后续的实现比较简单，按照 LRU-K 算法的逻辑简单遍历查找最早的访问时间戳即可。\nevictable 等变量的变化比较简单，按照文档来就可以，这里就不再多说了。\nBuffer Pool Manager Instance Buffer Pool Manager Design 这个部分的代码不是很难，主要是需要理清各个函数的作用和关系。\nBuffer Pool Manager 里有几个重要的成员：\npages：buffer pool 中缓存 pages 的指针数组 disk_manager：框架提供，可以用来读取 disk 上指定 page id 的 page 数据，或者向 disk 上给定 page id 对应的 page 里写入数据 page_table：刚才实现的 Extendible Hash Table，用来将 page id 映射到 frame id，即 page 在 buffer pool 中的位置 replacer：刚才实现的 LRU-K Replacer，在需要驱逐 page 腾出空间时，告诉我们应该驱逐哪个 page free_list：空闲的 frame 列表 Buffer Pool Manager 给上层调用者提供的两个最重要的功能是 new page 和 fetch page。我们先理一理 Buffer Pool Manager 完成这两项工作的流程：\nNew Page\n上层调用者希望新建一个 page，调用 NewPgImp。\n如果当前 buffer pool 已满并且所有 page 都是 unevictable 的，直接返回。否则：\n如果当前 buffer pool 里还有空闲的 frame，创建一个空的 page 放置在 frame 中。 如果当前 buffer pool 里没有空闲的 frame，但有 evitable 的 page，利用 LRU-K Replacer 获取可以驱逐的 frame id，将 frame 中原 page 驱逐，并创建新的 page 放在此 frame 中。驱逐时， 如果当前 frame 为 dirty(发生过写操作)，将对应的 frame 里的 page 数据写入 disk，并重置 dirty 为 false。清空 frame 数据，并移除 page_table 里的 page id，移除 replacer 里的引用记录。 如果当前 frame 不为 dirty，直接清空 frame 数据，并移除 page_table 里的 page id，移除 replacer 里的引用记录。 在 replacer 里记录 frame 的引用记录，并将 frame 的 evictable 设为 false。因为上层调用者拿到 page 后可能需要对其进行读写操作，此时 page 必须驻留在内存中。\n使用 AllocatePage 分配一个新的 page id(从 0 递增)。\n将此 page id 和存放 page 的 frame id 插入 page_table。\npage 的 pin_count 加 1。\nFetch Page\n上层调用者给定一个 page id，Buffer Pool Manager 返回对应的 page 指针。调用 FetchPgImp。\n假如可以在 buffer pool 中找到对应 page，直接返回。\n否则需要将磁盘上的 page 载入内存，也就是放进 buffer pool。\n如果当前 buffer pool 已满并且所有 page 都是 unevictable 的，直接返回空指针。否则同 New Page 操作，先尝试在 free list 中找空闲的 frame 存放需要读取的 page，如果没有 frame 空闲，就驱逐一张 page。获得一个空闲的 frame。\n通过 disk_manager 读取 page id 对应 page 的数据，存放在 frame 中。在 replacer 里记录引用，将 evictable 设为 false，将 page id 插入 page_table，page 的 pin_count 加 1。\n流程还是比较简单的，总的来说就是 buffer pool 里没空位也腾不出空位，直接返回，暂时处理不了请求，如果有空位，就先用空位，没空位但可以驱逐，就驱逐一个 page 腾出空位。这样就可以在内存中缓存一个 page 方便上层调用者操作。同时，还需要同步一些信息，比如 page_table 和 replacer，驱逐 page 时，如果是 dirty page 也需要先将其数据写回 disk。\n接下来说说 pin 和 unpin。\n当上层调用者新建一个 page 或者 fecth 一个 page 时，Buffer Pool Manager 会自动 pin 一下这个 page。接下来上层调用者对这个 page 进行一系列读写操作，操作完之后调用 unpin，告诉 Buffer Pool Manager，这个 page 我用完了，你可以把它直接丢掉或者 flush 掉了（也不一定真的可以，可能与此同时有其他调用者也在使用这个 page，具体能不能 unpin 掉要 Buffer Pool Manager 在内部判断一下 page 的 pin_count 是否为 0）。调用 unpin 时，同时传入一个 is_dirty 参数，告诉 Buffer Pool Manager 我刚刚对这个 page 进行的是读操作还是写操作。需要注意的是，Buffer Pool Manager 不能够直接将 page 的 dirty flag 设为 is_dirty。假设原本 dirty flag 为 true，则不能改变，代表其他调用者进行过写操作。只有原本 dirty flag 为 false 时，才能将 dirty flag 直接设为 is_dirty。\n整个流程大概就是这样。把流程理清楚，注意一些变量的同步，还是比较简单的。\nSummary 整个 project 1 难度不算大，coding + debug 时间大概是 4 个小时左右。个人感觉难度最大的部分是 Extendible Hash Table，因为要进行一些比较 tricky 的位运算操作，我是真的有点玩不转。Buffer Pool Manager 部分的流程比较复杂，细节比较多，但认真按注释编写应该不会有什么问题。\n由于所有数据结构都是粗暴的一把大锁锁住，代码的性能不尽如人意，这里留个坑，之后有机会优化一下。\n","permalink":"https://blog.eleven.wiki/posts/cmu15-445-project1-buffer-pool-manager/","summary":"暂时战略放弃了 6.824 的 Lab4，来做做 CMU 新鲜出炉的 15-445 FALL 2022。\nResources https://15445.courses.cs.cmu.edu/fall2022 课程官网 https://github.com/cmu-db/bustub Bustub Github Repo https://www.gradescope.com/ 自动测评网站 GradeScope，course entry code: PXWVR5 https://discord.gg/YF7dMCg Discord 论坛，课程交流用 bilibili 有搬运的课程视频，自寻。 请不要将实现代码公开，尊重 Andy 和 TAs 的劳动成果！\nSet up Environment 实验需要 Linux 环境。虽说 Docker 什么的似乎也可以，但 Linux 总是更令人安心。为了方便测试，我选择了云服务器。\nJetBrains 家推出了新的 SSH 远程开发功能。本来想试试，结果 CLion 在 server 上足足吃了两三个 G 的内存，我 2 核 4G 的服务器不堪重负，还是老老实实用 vscode remote。\nDebug 推荐用 lldb，在 vscode 安装相关插件后体验很好。终于不用整天翻看 6.824 又臭又长的 log 了。\nOverview Project1 主要与 Bustub 的 storage manager 相关，分为三个部分：","title":"CMU15-445 Project1 Buffer Pool Manager"},{"content":"Introduction 摸了一段时间的鱼，去尝试用 SpringBoot + Vue 搭建一个前后端分离的 Web 应用。都说 CS 的学生不能只会用框架 CRUD，但更不能不会用框架 CRUD。大概做出来一个小 demo 后就没有再继续了。接着来做 6.824。\nLab3 和 Lab2 各有各的难处。Lab2 是 Raft 的核心，需要小心地处理各种 corner case。但好在有 Figure2 的引导，最终也是按照其严格的规定成功实现。Lab3 编码难度不如 Lab2，但没有详细的引导，留给个人自由发挥的空间更大 (更有可能不知道从何下手)。\nLab3 需要在 Raft 层上实现一个 fault-tolerant key-value service，满足强一致性，也就是线性一致性 (Linearizable Consistency)。线性一致性保证整个系统看起来好像只有一个副本，其中所有的操作都是原子性的。简单地说，线性一致性系统的读写操作有以下特征：\n读写并不能瞬间完成，而是在一个时间段内进行。 读在写开始前完成，读到的一定是旧值；读在写完成之后开始，读到的一定是新值。(读写操作无重合部分) 读写并发进行，即有重合部分时，既可能读到新值，也可能读到旧值。 一旦有一个客户端读取到了新值，那么之后的客户端一定也都会读取到新值。 关于线性一致性更加详细的定义和解析，可以阅读《数据密集型应用系统》(DDIA) 相关章节。\n先来看看整个系统的架构：\n整个 KV Service 由多个 Server 组成，每个 Server 包含一个 State Machine，具体在 lab 中是一个 KV 数据库；Server 还包含一个 Raft 节点。需要注意的是，各 Server 之间并不会直接通信，而是靠其 Raft 节点进行通信。\n整个系统的理想运行流程是：\nClient 通过 RPC 向 KV Service 发送请求，例如 Put(x,1) KV Service 将请求转发给当前拥有 Leader Raft 节点的 Server Leader Server 将请求包含的 command 传递给 Raft 层 Raft 层对 command 进行共识，生成相同的 log replica 在达成共识后，Raft 层将 command Apply 回 Server Server 收到 Raft 层的 Apply 后，将 command 应用到状态机，即状态机此时状态为 {x: 1} 成功应用至状态机后，Server 对 Client 的 RPC 进行回复，返回结果和错误码。 当系统能够正常运行时，一切看起来都很清晰美好。但是一旦出现问题，如节点挂掉、RPC丢包、网络分区等等，情况就变得比较复杂了。\nImplement 在 Lab3 中，我们主要需要实现的部分是 Client 、 Server 和 Server KV Database。\nKV Database 在 lab 中，KV 数据库并不是主要内容。因此直接用 Hashmap 模拟即可。键值均为 String。\ntype kvdb struct { m map[string]string } func (db *kvdb) put(key string, value string) { db.m[key] = value } func (db *kvdb) append(key string, value string) { db.m[key] += value } func (db *kvdb) get(key string) (string, Err) { if _, ok := db.m[key]; !ok { return \u0026#34;\u0026#34;, ErrNoKey } return db.m[key], OK } 在这里，我没有在 KV db 中使用单独的锁。因为这些操作在后续代码中都是互斥进行的，无需额外加锁。\nClient Client 也相对简单。Client 可向 Server 发送三种不同的 RPC：Put(key,value)， Append(key,arg)和 Get(key)。Put 和 Append 均为写请求，Get 为读请求。\n一开始，Client 并不知道 Leader Server 是哪台 Server。Client 可向随机一台 Server 发送 RPC 请求。假如请求的 Server 不是当前的 Leader Server，或者由于网络中断、Server Crash 等原因，无法与 Server 取得联系，则无限地尝试更换 Server 重新发送请求，直到请求成功被处理。这里有一个小优化，在得知 Leader Server 后，Client 可以保存 Leader 的 id，避免下次发起请求时又需要随机地选择一台 Server 多次尝试。\ntype Clerk struct { servers []*labrpc.ClientEnd seq int64 // write op index, increasing from 1 id int64 // client uuid leader int } func (ck *Clerk) Get(key string) string { args := GetArgs{ Key: key, ClientId: ck.id, } i := ck.leader defer func() { ck.leader = i }() for { reply := GetReply{} ok := ck.servers[i].Call(\u0026#34;KVServer.Get\u0026#34;, \u0026amp;args, \u0026amp;reply) if !ok || reply.Err == ErrWrongLeader || reply.Err == ErrTimeout { // cannot reach the server, or it\u0026#39;s a wrong leader: retry i = (i + 1) % len(ck.servers) continue } if reply.Err == ErrNoKey { return \u0026#34;\u0026#34; } return reply.Value } } func (ck *Clerk) PutAppend(key string, value string, op string) { args := PutAppendArgs{ Key: key, Value: value, Op: op, Seq: ck.seq, ClientId: ck.id, } i := ck.leader defer func() { ck.seq++ ck.leader = i }() for { reply := PutAppendReply{} ok := ck.servers[i].Call(\u0026#34;KVServer.PutAppend\u0026#34;, \u0026amp;args, \u0026amp;reply) if !ok || reply.Err == ErrWrongLeader || reply.Err == ErrTimeout { // cannot reach the server, or it\u0026#39;s a wrong leader: retry i = (i + 1) % len(ck.servers) continue } // successfully PutAppend return } } 关于 Client 还需要维护的另一些状态，如 id 和 seq，待会儿再讨论。\nServer Server 应该是 Lab3 中最为复杂的部分。我们先讨论一切正常的情况下 Server 的设计。\n读写 RPC Handler 在接收到 Client 的请求后，通过调用 raft.Start() 将请求包含的 command 传递到 Raft 层，达成共识。当然，如果当前 Server 不为 Leader，则向 Client 返回 ErrWrongLeader 错误，Client 在收到回复后重新尝试向另一台 Server 发起请求。Raft 层达成共识后，通过 applyCh 通知 Server 该 command 已达成共识。\n在一开始，可能会这样设计 Server：\n_, _, isLeader := kv.rf.Start(op) // push op to raft layer to reach the agreement if !isLeader { reply.Err = ErrWrongLeader return } \u0026lt;- applyCh // agreed! apply to statemachine kv.apply(op) 对于不出错的单 Client，这样设计似乎没有问题。但如果有多个 Client 并行地向 Server 发起请求时，就显然不能保证从 applyCh 传回的数据恰好是此前提交的 command 了。为了解决这个问题，我们需要在 Server 中对特定的 command 进行等待。如何区分不同的 command？用 command 在 raft log 中的 index 区分即可。Server 需要维护一张 Map，Key 为 index，Value 为 Server 等待此 index 对应 command 的 channel。\ntype Result struct { value string err Err } notifyCh := map[int]chan Result 在 Raft 层，command 一定是按序向 applyCh 传输的。为了能够按序将 command 应用至状态机，Server 应起一后台 goroutine 监听 applyCh，对需要 apply 的 command 进行互斥的处理。同时，这个 goroutine 也负责将 applyCh 传来的信息转发给对应的正在阻塞等待的 RPC Handler：\nfunc (kv *KVServer) notifier() { for !kv.killed() { select { case msg := \u0026lt;-kv.applyCh: op := msg.Command.(Op) result := kv.apply(op)\t// apply to state machine index := msg.CommandIndex ch := kv.getNotifyCh(index)\tch \u0026lt;- result\t// notify the blocked server } } } func (kv *KVServer) getNotifyCh(index int) chan Result { kv.mu.Lock() defer kv.mu.Unlock() if _, ok := kv.notifyCh[index]; !ok { kv.notifyCh[index] = make(chan Result) } return kv.notifyCh[index] } Server 阻塞的代码改写如下：\nindex, _, isLeader := kv.rf.Start(op) if !isLeader { reply.Err = ErrWrongLeader return } ch := kv.getNotifyCh(index) select { case result := \u0026lt;-ch: // agreed! reply to client reply.Value, reply.Err = result.value, result.err case \u0026lt;-time.After(AGREE_TIMEOUT): // too slow response, reply to client and let it retry another server reply.Value, reply.Err = \u0026#34;\u0026#34;, ErrTimeout } go func() { // asynchronously release notifying channel kv.delNotifyCh(index) }() 需要注意的是，如果 Raft 层长时间无法完成共识 (由于网络分区等原因)，不要让 Server 一直阻塞。及时向 Client 返回 Timeout 错误，使其重新选择另一台 Server 重试。\n这样一来，多个 Client 并行发送请求的情况似乎也可以应对了。\n然而我们要实现的系统有一个重要的性质，fault-tolerant。目前为止，fault 还没有出现。\n实际上，Raft 层的各种 fault 我们在 lab2 中已经妥善处理了，因此我们主要需要关注的是 Server 层的 fault。首先不考虑 Server 直接挂掉的情况 (需要在 lab3B 中用 snapshot 解决)，那么剩下的就是 Client 和 Server 之间的 RPC 丢失问题了。\n假如 Client 向 Server 发送请求，因网络问题 Server 无法接收，这种情况 Server 无需应对 (也无力应对)，让 Client 自己慢慢重试就好。比较严重的问题是，Client 发送的请求 Server 成功接收，Server 也将请求中的 command 成功在 Raft 层达成共识并应用至状态机，然而在回复 Client 时出现了问题，RPC 回复丢失。这样就有可能导致一次请求多次重复提交的情况。比如下面一种简单的情况：\nClient 向 Server 发送 Append(x, 1) 的请求 Server 成功接收，Raft 层达成共识，应用至状态机。此时状态机状态 {x: 1} 由于网络原因，Server 向 Client 返回的结果丢失 Client 苦苦等待，也没有收到 Server 返回的结果，于是超时重试。绕了一圈后又回到了这个 Server (此 Server 仍为 Leader) Client 又向 Server 发送 Append(x, 1) 的请求，Server 成功接收，Raft 层达成共识，应用至状态机。此时状态机状态 {x: 11} 这次 Server 成功向 Client 返回了结果。 Client 成功收到了返回的结果，结束请求。然而原本的 Append(x, 1) 请求，造成了 Append(x, 11) 的后果。 出现这种情况的根本原因是，Raft 层允许同样的 command commit 多次 (Raft 层并不知道这是不是相同的 command，只要有 command 来，就尝试共识)，但实际上，同样的 command 只能 apply 一次。这就需要我们在 Server 层对请求进行去重。\n上面只介绍了 Append 请求的情况，Put 请求也类似。虽然在只有一个 Client 时，Put 请求多次执行不会改变结果，但如果有多个 Client，重复的 Put 请求也可能造成互相覆盖的后果。因此也需要进行去重。\n至于 Get 请求，多次重复并不会改变状态机的状态，无需进行去重处理。\n说到 Get 请求，在这里小小地偏一下题：\n按我们目前的实现，Get/Put/Append 请求均需先推至 Raft 层达成共识，记录在 Raft 层的 Log 中。然而 Get 请求并不会改变系统的状态，记录在 Log 中，对崩溃后回放 Log 恢复数据也没有什么帮助。那么实际上是不是不需要将 Get 请求传入 Raft 层进行共识呢？是的。并且这样会使系统效率更高。那么为什么我们要将 Get 请求也传入 Raft 层呢？这么做实际上是为了简化 KV Service 的实现难度。KV Service 要求我们永远不在 minority 中读取数据，因为这样可能会破坏线性一致性。假如我们不将 Get 传入 Raft 层，直接读取 Leader Server 状态机中的数据，试想下面这种情况：\n一共有 5 台 Server。一开始，Server1 为 Leader，Client 发送了一些请求，Raft 成功共识。 此后，Server1、Server2 与 Server3、Server4、Server5 由于网络问题被划分成两个部分。第一部分中，Server1 仍认为自己是 Leader。第二部分中，Server3 成功当选 Leader。 Server3 又接收了一些来自 Client 的请求，且在 Server3、Server4、Server5 间达成了共识。 有两个 Client 希望 Get 同一个 key： Client1 首先联系了 Server1，Server1 认为它自己是 Leader (实际已经 outdated)，便向 Client1 返回了 outdated value。 Client2 首先联系 Server3，Server3 向其返回了 updated value。 这两个 Get 操作间并没有写操作，却读到了不同的数据，违背了线性一致性。 为什么将 Get 传入 Raft 进行共识就可以避免这种错误？依然考虑上述情况：Server1 在接收到 Client1 的 Get 请求后，将其传入 Raft 层试图达成共识。然而 Server1 只能获得 Server2 的响应，无法将 Get 请求同步到大多数节点上，所以迟迟无法达成共识，Server 层也会被长期阻塞。Client1 久久等不到答复，便会更换 Server 重新进行请求，此时就会找到新的 Leader Server3 并成功执行 Get 请求。所以，将 Get 请求一同传入 Raft 层是最简单地避免读取到 minority 数据的方法。\nRaft 论文在 session 8 中提到了 read-only operations 等优化，避免将 Get 写入 Log，同时解决了可能获取 outdated 数据的问题。可以自行参考。\n去重具体的执行方式，就和之前在 Client 中还没有讲到的 id、seq 等变量有关了。\nid 是 Client 的 uuid，用于标识不同的 Client，直接用 skeleton code 中的 nrand() 方法生成即可 (测试时开不了那么多 Client，碰撞概率极低，可以凑合当uuid用)。seq 则是 Client 写操作的最大操作数，从 1 开始递增。每当 Client 完成一次写操作，就将 seq 加 1。\nServer 端则维护一张 map，用于记录不同 Client 成功应用至状态机的最大 seq 数：\nmaxSeq := map[int64]int64 在遇到来自 client x 的 y seq 请求时，如果\nmaxSeq[x] \u0026gt;= y 就表明这是一次重复的请求，需要进行拦截。\n此时又有新的问题出现了，我们应该在哪里拦截重复的请求，在哪里更新 maxSeq？\n我一开始的想法是，直接在 RPC handler 的最开始判断请求是否重复，若是重复请求则直接拦截并返回。并在 RPC handler 返回前更新 maxSeq。然而这种处理方法存在问题。试想如下情况：\nClient 首先向 Leader Server1 发起 Append 请求。Server1 成功完成共识并将请求应用至状态机，也更新了 maxSeq。但在返回时 RPC 结果丢失。 此时，恰好 Server1 由于崩溃或网络隔离等原因，失去 Leader 身份，Server2 当选 Leader。 由于 RPC 结果丢失，Client 长时间得不到响应，便尝试更换 Server 重新发起请求。 Client 向 Server2 发起了同样的 Append 请求。由于 Server2 的 maxSeq 中并没有此 Client 此 Seq 的信息 (上次仅是存储在了 Server1 的 maxSeq)，于是 Server2 再次执行了请求。也导致了一次请求多次应用的后果。 出现这种情况的根本原因是 Server 并不会直接联系，不同 Server 的 maxSeq 无法共享，因此在 Client 切换 Server 提交重复请求时，Server 无法察觉。\n解决方法也比较简单：在 command 达成共识后，将 command 应用至状态机前，对 command 进行去重。并在成功应用 command 后，更新 maxSeq。即 maxSeq 实际上由状态机维护。这样做能成功的原因是，不同的 Server 通过 Raft 层的交流，间接地共享了 maxSeq。所有请求都需要先尝试应用至状态机，而状态机维护的 maxSeq 恰好可以拦截试图应用的重复请求。\nfunc (kv *KVServer) apply(op Op) Result { result := Result{} if op.T == \u0026#34;Get\u0026#34; { result.value, result.err = kv.db.get(op.Key) } else if op.T == \u0026#34;Put\u0026#34; { if kv.maxSeq[op.ClientId] \u0026lt; op.Seq { kv.db.put(op.Key, op.Value) kv.maxSeq[op.ClientId] = op.Seq } result.err = OK } else { if kv.maxSeq[op.ClientId] \u0026lt; op.Seq { kv.db.append(op.Key, op.Value) kv.maxSeq[op.ClientId] = op.Seq } result.err = OK } return result } 到此为止，似乎我们的 KV Service 已经完美无缺了。当时我就是这么认为的，然而它还存在两个逻辑上的小问题 TAT\n我们用来转发 applyCh 信息的 notifier 协程是这样的：\nfunc (kv *KVServer) notifier() { for !kv.killed() { select { case msg := \u0026lt;-kv.applyCh: op := msg.Command.(Op) result := kv.apply(op)\t// apply to state machine index := msg.CommandIndex ch := kv.getNotifyCh(index)\tch \u0026lt;- result\t// notify the blocked server } } } 需要意识到的是，不是所有 Server 都是 Leader 节点，Follower 节点也会通过 applyCh 向 Server 层转递需要 apply 至状态机的数据。此时 Server 层并没有 RPC Handler 在等待 applyCh 的数据。如果我们仍尝试获取对应的 notifyCh 并转发数据，则会造成 notifier 的无限阻塞。改写如下：\nfunc (kv *KVServer) notifier() { for !kv.killed() { select { case msg := \u0026lt;-kv.applyCh: op := msg.Command.(Op) result := kv.apply(op)\t// apply to state machine if _, isLeader := kv.rf.GetState(); !isLeader { continue } index := msg.CommandIndex ch := kv.getNotifyCh(index)\tch \u0026lt;- result\t// notify the blocked server } } } Server 不为 Leader 的情况已经解决。假如 Server 当前是 Leader，有没有可能部分 applyCh 传来的数据也无需转发呢？也有可能，最简单的情况就是新当选的 Leader 的 Log 中还存在已提交未应用的 command。将这个 command 传入 Server 层后，按照上面的写法，也会尝试向并不存在的 RPC Handler 转发数据并造成阻塞。这种情况解决起来也比较简单，不属于当前 term 的 command 无需转发，直接给状态机应用就可以了。\nfunc (kv *KVServer) notifier() { for !kv.killed() { select { case msg := \u0026lt;-kv.applyCh: op := msg.Command.(Op) result := kv.apply(op) if term, isLeader := kv.rf.GetState(); !isLeader || term != msg.CommandTerm { continue } index := msg.CommandIndex ch := kv.getNotifyCh(index) ch \u0026lt;- result } } } 到这里 Lab3A 的要求已经完成了。\nTest \u0026amp; Debug 上面不断改错的经历大致就是我在测试集中不断 fail 并修改的过程。Lab 给出的测试集还是比较详尽的，可以测出各方面的细节。其中有一个测试集比较奇怪：\nTest: ops complete fast enough (3A) 这个测试是需要 command 达成共识并应用至状态机的速度足够快，每次心跳间隔 (100ms) 中至少需要完成 3 次共识。在 Lab2 中，我们已经在每次 Start(command) 时都提起一次复制请求，用不同的 replicator 向各个节点并行地不断尝试复制 Log，而不是完全靠心跳进行复制 (基本每次心跳间隔只能完成一次共识)。按理来说应该能够轻松通过，然而测试结果总是超时。\n我试了试不带 -race 标识的测试，结果很意外，仅仅 3s 左右就通过了测试，而带上 -race 标识足足需要 40s 左右。到这里其实已经基本可以猜到是什么问题了：锁的竞争过于激烈，-race 标识会进行 Data race 的检测， 严重地影响了系统的性能。\n我又在 test 中加入了对 goroutine 数量的监控，发现 goroutine 数量不断增长，高的时候可以达到 300+ goroutine。虽说 goroutine 号称是轻松开启上万个，但这么高的 goroutine 数量显然还是有点问题。\n之后经过排查，发现大量的 goroutine 卡在了 Lab2 Raft 层向 applier 发送 apply 请求的 goroutine 上：\ngo func() { rf.applyCh \u0026lt;- msg }() 这样就比较好解释了，Client 短时间内发起了大量的请求，而 applier 只有一个，大量尝试传递给 applyCh 的 msg 阻塞，导致协程数过大。\n因此需要将 Lab2 中过多的重复 msg 拦截，做法和 replicator 中类似，或者改用 sync.Cond，用 Signal 不阻塞的性质来实现。\nLab3A 部分到这里结束，接下来讲讲 Lab3B。\nSnapshot 前面的工作做好后，加上一个 Snapshot 其实比较简单，在 notifier 中根据 RaftStateSize() 和 MaxStateSize 的大小关系判断一下是否需要进行一次 Snapshot，并且增加一个 commandValid == false 的分支将 Raft 层传递的 Snapshot 应用至状态机就可以了。\n需要注意的是，Snapshot 不仅需要保存 kv database 的信息，还需要保存 maxSeq。因为改变了状态机的状态，就需要状态机相关的 maxSeq 信息来拦截重复请求。在应用 Snapshot 时，状态机状态发生改变，所以也需要将 maxSeq 与 Leader 的 maxSeq 进行同步。\nSummary Lab3 整体坐下来难度比 Lab2 小一些，debug 也很折磨人就是了。其间也遇到了不少概率极低但匪夷所思的小问题，或许 Raft 层还是不太对？不过也不想再去处理了，感觉分布式系统的 debug 更像是一种体力活。\n","permalink":"https://blog.eleven.wiki/posts/mit6.824-lab3-kvservice/","summary":"Introduction 摸了一段时间的鱼，去尝试用 SpringBoot + Vue 搭建一个前后端分离的 Web 应用。都说 CS 的学生不能只会用框架 CRUD，但更不能不会用框架 CRUD。大概做出来一个小 demo 后就没有再继续了。接着来做 6.824。\nLab3 和 Lab2 各有各的难处。Lab2 是 Raft 的核心，需要小心地处理各种 corner case。但好在有 Figure2 的引导，最终也是按照其严格的规定成功实现。Lab3 编码难度不如 Lab2，但没有详细的引导，留给个人自由发挥的空间更大 (更有可能不知道从何下手)。\nLab3 需要在 Raft 层上实现一个 fault-tolerant key-value service，满足强一致性，也就是线性一致性 (Linearizable Consistency)。线性一致性保证整个系统看起来好像只有一个副本，其中所有的操作都是原子性的。简单地说，线性一致性系统的读写操作有以下特征：\n读写并不能瞬间完成，而是在一个时间段内进行。 读在写开始前完成，读到的一定是旧值；读在写完成之后开始，读到的一定是新值。(读写操作无重合部分) 读写并发进行，即有重合部分时，既可能读到新值，也可能读到旧值。 一旦有一个客户端读取到了新值，那么之后的客户端一定也都会读取到新值。 关于线性一致性更加详细的定义和解析，可以阅读《数据密集型应用系统》(DDIA) 相关章节。\n先来看看整个系统的架构：\n整个 KV Service 由多个 Server 组成，每个 Server 包含一个 State Machine，具体在 lab 中是一个 KV 数据库；Server 还包含一个 Raft 节点。需要注意的是，各 Server 之间并不会直接通信，而是靠其 Raft 节点进行通信。\n整个系统的理想运行流程是：\nClient 通过 RPC 向 KV Service 发送请求，例如 Put(x,1) KV Service 将请求转发给当前拥有 Leader Raft 节点的 Server Leader Server 将请求包含的 command 传递给 Raft 层 Raft 层对 command 进行共识，生成相同的 log replica 在达成共识后，Raft 层将 command Apply 回 Server Server 收到 Raft 层的 Apply 后，将 command 应用到状态机，即状态机此时状态为 {x: 1} 成功应用至状态机后，Server 对 Client 的 RPC 进行回复，返回结果和错误码。 当系统能够正常运行时，一切看起来都很清晰美好。但是一旦出现问题，如节点挂掉、RPC丢包、网络分区等等，情况就变得比较复杂了。","title":"MIT6.824 Lab3 KVService"},{"content":"MIT6.824 NOTES Lecture 1 Introduction ","permalink":"https://blog.eleven.wiki/posts/mit6.824-notes/","summary":"MIT6.824 NOTES Lecture 1 Introduction ","title":"MIT6.824 Notes"},{"content":"趁着暑假有空，把鸽了很久的 MIT6.824 做一下。Lab1 是实现一个 Map-Reduce，因为和 Raft 主线关系不大（因为懒），就略过了。另外，这次尝试实现一个 part 就来记录相关的内容，以免在全部实现后忘记部分细节（以免之后太懒不想写）。因此，不同 part 的代码会变化，请以最终版本的代码为准（但保证每一 part 的代码可以正常通过绝大部分相应的测试）。同时，在写下某一 part 的记录时，我对 Raft 的整体把握也难免有所不足。\nResources Course\u0026rsquo;s Page 课程主页 Students\u0026rsquo; Guide to Raft 一篇引导博客 Debugging by Pretty Printing debug 技巧，强烈推荐阅读和运用 Raft Q\u0026amp;A 关于 Raft 的一些 Q\u0026amp;A Raft Visualization Raft 动画演示 In Search of an Understandable Consensus Algorithm Raft 论文 Lab2A Raft Leader Election Lab2A 实现时间为6.22~6.24。\nLab2A 主要实现 Raft 的选主过程，包括选举出 Leader 和 Leader 通过心跳维持身份。\nDesign 首先是选主过程的状态机模型：\n接下来是 Raft 论文中最为重要的 Figure 2:\nFigure 2 有许多关于日志复制等其他部分的内容，在这里暂时先不考虑（但当然还是推荐先整体熟悉 Raft 所有内容后再开始编码）。关于选举部分的内容已经全部在图中标出。一个一个看：\nState 每个 Raft 节点需要维护的状态：\ncurrentTerm 此节点的任期。 votedFor 在当前任期内，此节点将选票投给了谁。一个任期内，节点只能将选票投给某一个节点。因此当节点任期更新时要将 votedfor 置为 null。 AppendEntries RPC 在领导选举的过程中，AppendEntries RPC 用来实现 Leader 的心跳机制。节点的 AppendEntries RPC 会被 Leader 定期调用。\nArgs\nterm Leader 的任期。 leaderId Client 可能将请求发送至 Follower 节点，得知 leaderId 后 Follower 可将 Client 的请求重定位至 Leader 节点。因为 Raft 的请求信息必须先经过 Leader 节点，再由 Leader 节点流向其他节点进行同步，信息是单向流动的。在选主过程中，leaderId 暂时只有 debug 的作用。 Reply\nterm 此节点的任期。假如 Leader 发现 Follower 的任期高于自己，则会放弃 Leader 身份并更新自己的任期。 success 此节点是否认同 Leader 发送的心跳。 Receiver Implementation\n当 Leader 任期小于当前节点任期时，返回 false。 否则返回 true。 RequestVote RPC RequestVote RPC 会被 Candidate 调用，以此获取选票。\nArgs\nterm Candidate 的任期 candidateId Reply\nterm 此节点的任期。假如 Candidate 发现 Follower 的任期高于自己，则会放弃 Candidate 身份并更新自己的任期。 voteGranted 是否同意 Candidate 当选。 Receiver Implementation\n当 Candidate 任期小于当前节点任期时，返回 false。 如果 votedFor 为 null（即当前任期内此节点还未投票）或者 votedFor为 candidateId（即当前任期内此节点已经向此 Candidate 投过票），则同意投票；否则拒绝投票。 Rules for Servers All Servers\n如果来自其他节点的 RPC 请求中，或发给其他节点的 RPC 的回复中，任期高于自身任期，则更新自身任期，并转变为 Follower。 Followers\n响应来自 Candidate 和 Leader 的 RPC 请求。 如果在 election timeout 到期时，Follower 未收到来自当前 Leader 的 AppendEntries RPC，也没有收到来自 Candidate 的 RequestVote RPC，则转变为 Candidate。 Candidates\n转变 Candidate时，开始一轮选举： currentTerm++ 为自己投票（votedFor = me） 重置 election timer 向其他所有节点并行发送 RequestVote RPC 如果收到了大多数节点的选票（voteCnt \u0026gt; n/2），当选 Leader。 在选举过程中，如果收到了来自新 Leader 的 AppendEntries RPC，停止选举，转变为 Follower。 如果 election timer 超时时，还未当选 Leader，则放弃此轮选举，开启新一轮选举。 Leaders\n刚上任时，向所有节点发送一轮心跳信息 此后，每隔一段固定时间，向所有节点发送一轮心跳信息，重置其他节点的 election timer，以维持自己 Leader 的身份。 至此，选主的流程已经比较清晰，接下来是具体的实现。\nImplementation 需要实现的结构体不再赘述，按照 Figure2 来就行。\n首先实现两个RPC:\nAppendEntries RPC func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { rf.mu.Lock() defer rf.mu.Unlock() if args.Term \u0026lt; rf.currentTerm { // Reply false if term \u0026lt; currentTerm reply.Success = false reply.Term = rf.currentTerm\treturn } if args.Term \u0026gt; rf.currentTerm { // If RPC request contains term T \u0026gt; currentTerm: // set currentTerm = T, convert to follower rf.currentTerm = args.Term rf.votedFor = -1 rf.state = FOLLOWER } // received AppendEntries RPC from current leader, reset election timer rf.electionTimer.Reset(randomElectionTimeout()) reply.Success = true reply.Term = rf.currentTerm } RequestVote RPC func (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { rf.mu.Lock() defer rf.mu.Unlock() if args.Term \u0026lt; rf.currentTerm { // Reply false if term \u0026lt; currentTerm reply.VoteGranted = false reply.Term = rf.currentTerm return } if args.Term \u0026gt; rf.currentTerm { // If RPC request contains term T \u0026gt; currentTerm: // set currentTerm = T, convert to follower rf.currentTerm = args.Term rf.votedFor = -1 rf.state = FOLLOWER } if rf.votedFor != -1 \u0026amp;\u0026amp; rf.votedFor != args.CandidateId { // If votedFor is null or candidateId, grant vote; otherwise reject reply.VoteGranted = false reply.Term = rf.currentTerm return } // grant vote to candidate, reset election timer rf.electionTimer.Reset(randomElectionTimeout()) rf.votedFor = args.CandidateId reply.VoteGranted = true reply.Term = rf.currentTerm } 可以看到两个 RPC 的实现与 Figure 2 中的规则完全一致。依次实现即可。需要注意的是，处理 RPC 的整个过程中都需要持有锁。另外，在更新节点任期时，一定要同步将votedFor 置为 null。\n实现完两个 RPC 后，再实现较为复杂的 election 和 heartbeat 过程。\nElection 在节点的 election timer 过期后，开始选举。因此，节点需要有一个监控 electon timer 的 go routine，ticker。\nfunc (rf *Raft) ticker() { for !rf.killed() { select { case \u0026lt;-rf.electionTimer.C: rf.mu.Lock() if rf.state == LEADER { rf.mu.Unlock() break } rf.state = CANDIDATE rf.mu.Unlock() go rf.startElection() } } } 选举过程的 go routine 为 startElection。为什么将选举过程也作为一个 go routine，而不是阻塞地调用函数？因为在规则中提到过，如果 election timer 超时时，Candidate 还未当选 Leader，则放弃此轮选举，开启新一轮选举。\n接下来看实际负责选举过程的 go routine， startElection。\nfunc (rf *Raft) startElection() { rf.mu.Lock() rf.currentTerm++ // Increment currentTerm rf.votedFor = rf.me // Vote for self rf.electionTimer.Reset(randomElectionTimeout()) // Reset election timer rf.mu.Unlock() args := RequestVoteArgs{CandidateId: rf.me} rf.mu.RLock() args.Term = rf.currentTerm rf.mu.RUnlock() voteCh := make(chan bool, len(rf.peers)-1) for i := range rf.peers { // Send RequestVote RPCs to all other servers if i == rf.me { // in PARALLEL continue } go func(i int) { reply := RequestVoteReply{} if ok := rf.sendRequestVote(i, \u0026amp;args, \u0026amp;reply); !ok { voteCh \u0026lt;- false return } rf.mu.Lock() if reply.Term \u0026gt; rf.currentTerm { // If RPC response contains term T \u0026gt; currentTerm: // set currentTerm = T, convert to follower rf.currentTerm = reply.Term rf.votedFor = -1 rf.state = FOLLOWER rf.mu.Unlock() return } rf.mu.Unlock() voteCh \u0026lt;- reply.VoteGranted }(i) } voteCnt := 1 voteGrantedCnt := 1 for voteGranted := range voteCh { rf.mu.RLock() state := rf.state rf.mu.RUnlock() if state != CANDIDATE { break } if voteGranted { voteGrantedCnt++ } if voteGrantedCnt \u0026gt; len(rf.peers)/2 { // gain over a half votes, switch to leader rf.mu.Lock() rf.state = LEADER rf.mu.Unlock() go rf.heartbeat() break } voteCnt++ if voteCnt == len(rf.peers) { // election completed without getting enough votes, break break } } } 使用 n-1 个协程向其他节点并行地发送 RequestVote 请求。协程获得 response 后，向 voteCh 发送结果，startElection 协程进行结果统计。统计过程中，若发现失去了 Candidate 身份，则停止统计。若获得票数过半，则成功当选 Leader，启动 heartbeat 协程。若所有成员已投票，且未当选 Leader，则退出统计。\n要注意的是，需要确保所有不再使用的 go routine 能够正常退出，避免占据资源。\n成功当选 Leader 后，开始发送心跳。\nHeartbeat func (rf *Raft) heartbeat() { wakeChPool := make([]chan struct{}, len(rf.peers)) doneChPool := make([]chan struct{}, len(rf.peers)) // allocate each peer with a go routine to send AppendEntries RPCs for i := range rf.peers { if i == rf.me { continue } wakeChPool[i] = make(chan struct{}) doneChPool[i] = make(chan struct{}) go func(i int) { // replicator go routine for { select { case \u0026lt;-wakeChPool[i]: args := AppendEntriesArgs{LeaderId: rf.me} reply := AppendEntriesReply{} rf.mu.RLock() args.Term = rf.currentTerm rf.mu.RUnlock() go func() { if ok := rf.sendAppendEntries(i, \u0026amp;args, \u0026amp;reply); !ok { return } rf.mu.Lock() if reply.Term \u0026gt; rf.currentTerm { rf.currentTerm = reply.Term rf.votedFor = -1 rf.state = FOLLOWER rf.mu.Unlock() return } rf.mu.Unlock() }() case \u0026lt;-doneChPool[i]: return } } }(i) } broadcast := func() { for i := range rf.peers { if i == rf.me { continue } go func(i int) { wakeChPool[i] \u0026lt;- struct{}{} }(i) } } broadcast() rf.heartbeatTimer = time.NewTimer(HEARTBEAT_INTERVAL) for { \u0026lt;-rf.heartbeatTimer.C if rf.killed() || !rf.isLeader() { break } rf.heartbeatTimer.Reset(HEARTBEAT_INTERVAL) broadcast() } // killed or no longer the leader, release go routines for i := range rf.peers { if i == rf.me { continue } go func(i int) { doneChPool[i] \u0026lt;- struct{}{} }(i) } } heartbeat 协程首先为每个节点分配一个 replicator 协程，每个 replicator 协程负责向一个特定的节点发送 AppendEntries RPC。\n这些协程由 wakeChPool[i] 唤醒。实际上也可以用 sync.Cond 条件变量实现，但我不太会用，所以简单地用一组 channel 模拟。\n初始化这些协程后，heartbeat 协程首先进行一个初始的 broadcast，对应 Leader 刚当选时发出的一轮心跳。broadcast 即通过 wakeChPool 唤醒所有 replicator 协程，向所有节点发出一次心跳。\n此后，heartbeat 协程初始化一个 heartbeatTimer，并且在每次 heartbeatTimer 到期时，进行一次 broadcast，通知所有 replicator 协程发送一次心跳。这里需要注意的是，如果节点已经被 kill 或者不再是 Leader，需要中断对 heartbeatTimer 的监听，并且释放所有 replicator 协程。\n至此，选主过程和心跳成功实现。\nDevil in the details Lab2A 难度不算大，然而我还是被一个细节卡住了挺久。\n在 6.824 Raft 实验中，已经给我们提供了 RPC 调用的方法，即\nrf.peers[server].Call(\u0026#34;Raft.RPCName\u0026#34;, args, reply) 其注释提到，\nCall() is guaranteed to return (perhaps after a delay) except if the handler function on the server side does not return. Thus there is no need to implement your own timeouts around Call().\nCall() 是确保一定会返回的，除非在被调用的RPC中阻塞，否则即使模拟的网络中断，Call() 也会正常返回 false。因此不需要再为 Call() 设置一个 Timeout 限制。\n然而，经过测试，Call() 的确会确保返回，但返回的时间可能会非常长（3到4秒，具体数值要阅读 labrpc 源码，我还没有仔细阅读）。因此，在 replicator 协程中，每次发送心跳，我们还要再启动一个协程，将 sendAppendEntries 放在此协程中运行，避免哪怕只有几秒钟的阻塞。因为在这几秒中，Leader 可能又发送了新的 heartbeat，或者 Leader 不再是 Leader。\ngo func(i int) { // replicator go routine for { select { case \u0026lt;-wakeChPool[i]: ... go func() { // launch a new go routine to run sending RPC if ok := rf.sendAppendEntries(i, \u0026amp;args, \u0026amp;reply); !ok { return } }() case \u0026lt;-doneChPool[i]: return } ... } }(i) Summary 个人感觉 Lab2A 难度最大的地方在于合理控制各个 go routine 的生命周期。锁倒是暂时没碰到什么问题，直接一股脑地把可能存在 data race 的地方全部锁上并及时释放就好。整个选主过程的 go routine 生命周期如下：\nLab2A Leader Election 完成。\nLab2B Raft Log Replication Lab2B 开始于 6.28。结束于7.7。\n和 Raft 最核心的部分缠斗了一个多星期，终于敢说完成了一个较为稳定的版本，千次测试无一 fail。这段时间摸摸鱼，陪陪女朋友，玩玩游戏（和朋友们一起玩一款叫 Raft 的海上生存游戏，挺巧），无聊的时候再看看 fail 掉的 log，暑假嘛，开心最重要。\n关于 Lab2B 感触最深的就是 Students\u0026rsquo; Guide to Raft 里的这两段话：\nAt first, you might be tempted to treat Figure 2 as sort of an informal guide; you read it once, and then start coding up an implementation that follows roughly what it says to do. Doing this, you will quickly get up and running with a mostly working Raft implementation. And then the problems start.\nInevitably, the first iteration of your Raft implementation will be buggy. So will the second. And third. And fourth.\n完成第一版可以单次 pass 的代码大概用了5个小时左右，接下来信心满满地进行千次测试。然而随后的大部分时间，我基本都在试图从各种诡异的 log 找出出现概率极低的难以复现的 Bug。\nDesign 首先还是 Figure 2：\nLab2B 中需要完成 Figure 2 中余下的所有内容。顺带一提的是，Figure 2 与其说是一个 Raft 行为的汇总，更像是一个 coding 的 instruction。Figure 2 中很多地方直接给出了代码的具体行为，而不是给出比较抽象和模糊的规则。这样的好处是，coding 更加简单了，严格遵守 Figure 2 即可；但也有一定的坏处，可能实现完所有部分后，学生（特指我）还是对 Raft 的行为，设计和一致性证明等等比较模糊，仅是机械地遵循了 Figure 2 中给出的规则。下面还是一个一个来介绍：\nState log[] 即日志，每条 Entry 包含一条待施加至状态机的命令。Entry 也要记录其被发送至 Leader 时，Leader 当时的任期。Lab2B 中，在内存存储日志即可，不用担心 server 会 down 掉，测试中仅会模拟网络挂掉的情景。 commitIndex 已知的最高的已提交的 Entry 的 index。被提交的定义为，当 Leader 成功在大部分 server 上复制了一条 Entry，那么这条 Entry 就是一条已提交的 Entry。 lastApplied 最高的已应用的 Entry 的 index。已提交和已应用是不同的概念，已应用指这条 Entry 已经被运用到状态机上。已提交先于已应用。同时需要注意的是，Raft 保证了已提交的 Entry 一定会被应用（通过对选举过程增加一些限制，下面会提到）。 commitIndex 和 lastApplied 分别维护 log 已提交和已应用的状态，当节点发现 commitIndex \u0026gt; lastApplied 时，代表着 commitIndex 和 lastApplied 间的 entries 处于已提交，未应用的状态。因此应将其间的 entries 按序应用至状态机。\n对于 Follower，commitIndex 通过 Leader AppendEntries RPC 的参数 leaderCommit 更新。对于 Leader，commitIndex 通过其维护的 matchIndex 数组更新。\nnextIndex[] 由 Leader 维护，nextIndex[i] 代表需要同步给 peer[i] 的下一个 entry 的 index。在 Leader 当选后，重新初始化为 Leader 的 last log index + 1。 matchIndex[] 由 Leader 维护，matchIndex[i] 代表 Leader 已知的已在 peer[i] 上成功复制的最高 entry index。在 Leader 当选后，重新初始化为 0。 不能简单地认为 matchIndex = nextIndex - 1。\nnextIndex 是对追加位置的一种猜测，是乐观的估计。因此，当 Leader 上任时，会将 nextIndex 全部初始化为 last log index + 1，即乐观地估计所有 Follower 的 log 已经与自身相同。AppendEntries PRC 中，Leader 会根据 nextIndex 来决定向 Follower 发送哪些 entry。当返回失败时，则会将 nextIndex 减一，猜测仅有一条 entry 不一致，再次乐观地尝试。实际上，使用 nextIndex 是为了提升性能，仅向 Follower 发送不一致的 entry，减小 RPC 传输量。\nmatchIndex 则是对同步情况的保守确认，为了保证安全性。matchIndex 及此前的 entry 一定都成功地同步。matchIndex 的作用是帮助 Leader 更新自身的 commitIndex。当 Leader 发现一个 N 值，N 大于过半数的 matchIndex，则可将其 commitIndex 更新为 N（需要注意任期号的问题，后文会提到）。matchIndex 在 Leader 上任时被初始化为 0。\nnextIndex 是最乐观的估计，被初始化为最大可能值；matchIndex 是最悲观的估计，被初始化为最小可能值。在一次次心跳中，nextIndex 不断减小，matchIndex 不断增大，直至 matchIndex = nextIndex - 1，则代表该 Follower 已经与 Leader 成功同步。\nAppendEntries RPC Args\nprevLogIndex 添加 Entries 的前一条 Entry 的 index。 prevLogTerm prevLogIndex 对应 entry 的 term。 entries[] 需要同步的 entries。若为空，则代表是一次 heartbeat。需要注意的是，不需要特别判断是否为 heartbeat，即使是 heartbeat，也需要进行一系列的检查。因此本文也不再区分心跳和 AppendEntries RPC。 leaderCommit Leader 的 commitIndex，帮助 Follower 更新自身的 commitIndex。 Receiver Implementation\n若 Follower 在 prevLogIndex 位置的 entry 的 term 与 prevLogTerm 不同（或者 prevLogIndex 的位置没有 entry），返回 false。 如果 Follower 的某一个 entry 与需要同步的 entries 中的一个 entry 冲突，则需要删除冲突 entry 及其之后的所有 entry。需要特别注意的是，假如没有冲突，不能删除任何 entry。因为存在 Follower 的 log 更 up-to-date 的可能。 添加 Log 中不存在的新 entry。 如果 leaderCommit \u0026gt; commitIndex，令 commitIndex = min(leaderCommit, index of last new entry)。此即 Follower 更新 commitIndex 的方式。 RequestVote RPC Args\nlastLogIndex Candidate 最后一个 entry 的 index，是投票的额外判据。 lastLogTerm 上述 entry 的 term。 Receiver Implementation\n只有 Candidate 的 log 至少与 Receiver 的 log 一样新（up-to-date）时，才同意投票。Raft 通过两个日志的最后一个 entry 来判断哪个日志更 up-to-date。假如两个 entry 的 term 不同，term 更大的更新。term 相同时，index 更大的更新。\nRaft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs. If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the logs end with the same term, then whichever log is longer is more up-to-date.\n这里投票的额外限制是为了保证已经被 commit 的 entry 一定不会被覆盖。仅有当 Candidate 的 log 包含所有已提交的 entry，才有可能当选为 Leader。\nRules for Servers All Severs\n如果 commitIndex \u0026gt; lastApplied，lastApplied++，将 log[lastApplied] 应用到状态机。即前文提到的 entry 从已提交状态到已应用状态的过程。 Leaders\n如果收到了来自 client 的 command，将 command 以 entry 的形式添加到日志。在 lab2B 中，client 通过 Start() 函数传入 command。\n如果 last log index \u0026gt;= nextIndex[i]，向 peer[i] 发送 AppendEntries RPC，RPC 中包含从 nextIndex[i] 开始的日志。\n如果返回值为 true，更新 nextIndex[i] 和 matchIndex[i]。 如果因为 entry 冲突，RPC 返回值为 false，则将 nextIndex[i] 减1并重试。这里的重试不一定代表需要立即重试，实际上可以仅将 nextIndex[i] 减1，下次心跳时则是以新值重试。 如果存在 index 值 N 满足：\nN \u0026gt; commitIndex 过半数 matchIndex[i] \u0026gt;= N log[N].term == currentTerm 则令 commitIndex = N。\n这里则是 Leader 更新 commitIndex 的方式。前两个要求都比较好理解，第三个要求是 Raft 的一个特性，即 Leader 仅会直接提交其任期内的 entry。存在这样一种情况，Leader 上任时，其最新的一些条目可能被认为处于未被提交的状态（但这些条目实际已经成功同步到了大部分节点上）。Leader 在上任时并不会检查这些 entry 是不是实际上已经可以被提交，而是通过提交此后的 entry 来间接地提交这些 entry。这种做法能够 work 的基础是 Log Matching Property：\nLog Matching: if two logs contain an entry with the same index and term, then the logs are identical in all entries up through the given index.\n原文描述如下：\nTo eliminate problems like the one in Figure 8, Raft never commits log entries from previous terms by counting replicas. Only log entries from the leader’s current term are committed by counting replicas; once an entry from the current term has been committed in this way, then all prior entries are committed indirectly because of the Log Matching Property. There are some situations where a leader could safely conclude that an older log entry is committed (for example, if that entry is stored on every server), but Raft takes a more conservative approach for simplicity.\n这样简化了 Leader 当选的初始化工作，也成功避免了简单地通过 counting replicas 提交时，可能出现的已提交 entry 被覆盖的问题。\n到这里 Figure 2 基本介绍完毕。也大致解释了 Figure 2 中各种规则的缘由。Raft 论文中还有更多 Raft 的设计理念、Properties、安全性证明等内容，这里就不再赘述了。\nImplementation Lab2B 实现的难点应该在于众多的 corner case，以及理想情况与代码执行方式的差异，太多的线程和 RPC 让系统的复杂性骤升，未持有锁的时刻什么都有可能发生。另外还有一个令人纠结的地方，就是各种时机。例如，接收到了 client 的一个请求，什么时候将这条 entry 同步给 Follower？什么时候将已提交的 entry 应用至状态机？更新某一变量时，是起一线程轮询监听，还是用 channel 或者 sync.Cond 唤醒，还是采取 lazy 策略，问到我的时候再去计算？很多实现方式理论上都可以使用，或许也各有各的好处，限于时间，面对很多问题，我也只选择了一种我认为的比较容易实现的方式。\n这部分的代码相较于 Lab2A 有一些变动，除了 Lab2B 中新增的内容，主要是对投票过程进行了一些修改。\n先说 go routine 的使用。对于所有的初始节点（Follower 节点），包含如下后台 go routines：\nalerter：1个。监听 electionTimer 的超时事件和重置事件。超时事件发生时，Follower 转变为 Candidate，发起一轮选举。重置事件发生时，将 electionTimer 重置。 applier：1个。监听 applierCh channel，当节点认为需要一次 apply 时，向 applierCh 发送一次信号，applier 接收信号后会将当前 lastApplied 和 commitIndex 间的所有 entry 提交。 heartbeat：1个。监听 heartbeatTimer 的超时事件，仅在节点为 Leader 时工作。heartbeatTimer 超时后，Leader 立即广播一次心跳命令。 replicator：n-1 个，每一个对于一个 peer。监听心跳广播命令，仅在节点为 Leader 时工作。接收到命令后，向对应的 peer 发送 AppendEntries RPC。 所有节点仅拥有这 4 种长期执行的后台 go routines，以及若干短期执行任务的 go routines。接下来一个一个介绍。\nalerter alerter 代码如下：\nfunc (rf *Raft) alerter() { doneCh := rf.register(\u0026#34;alerter\u0026#34;) defer rf.deregister(\u0026#34;alerter\u0026#34;) for { FORLOOP: select { case \u0026lt;-rf.elecTimer.timer.C: rf.lock(\u0026#34;alerter\u0026#34;) if rf.state == LEADER { rf.unlock(\u0026#34;alerter\u0026#34;) break FORLOOP } select { case \u0026lt;-rf.elecTimer.resetCh: rf.elecTimer.reset() rf.unlock(\u0026#34;alerter\u0026#34;) break FORLOOP default: } // start a new election rf.state = CANDIDATE rf.startElection() rf.unlock(\u0026#34;alerter\u0026#34;) case \u0026lt;-rf.elecTimer.resetCh: if !rf.elecTimer.timer.Stop() { select { case \u0026lt;-rf.elecTimer.timer.C: default: } } rf.elecTimer.timer.Reset(randomElectionTimeout()) case \u0026lt;-doneCh: return } } } 看上去还是有点复杂，下面慢慢来解释。\n首先是 doneCh。关于在节点被 kill 后，如何让各个后台协程优雅退出，有不少方法。原始代码框架中给出了 killed() 方法，希望我们在后台协程长期运行的 for 循环中检查节点是否被 kill。但是这种方法不太好用，原因是 for 循环中常常阻塞在接收 channel 信号的语句。此时虽然进入了 for 循环，但节点可能在阻塞时被 kill，协程无法得知。\n我希望能够有一种方式，在 kill() 方法被调用后，直接通知所有的后台 goroutines 让其停止运行。\n最先想到的是 context。go 的 context 包可以用来处理类似的问题，如超时处理等等。基本思想是构建一颗 goroutine 树，父节点拥有关闭子节点的权力。但这里的场景稍微有点不同，不同的协程间不存在父子关系，只是 Raft 节点的不同后台协程。由 Raft 结构体管理，通过广播的方式通知所有协程较为合适。\n提到广播机制，就想到了 sync.Cond ，条件变量。sync.Cond 的 Broadcast() 方法似乎与需求很契合，但 sync.Cond 的阻塞形式是 cond.Wait()，而不是由 channel 阻塞，不太方便配合 select 语句进行多路复用。\n最后决定实现一个简单的 channel 广播方法。Raft 节点维护一个 doneCh map：\ndoneCh map[string]chan struct{} key 是字符串，为协程的名称。value 是 channel。\n在后台协程初始化时，调用rf.register() 方法：\nfunc (rf *Raft) register(name string) \u0026lt;-chan struct{} { rf.lock() rf.doneCh[name] = make(chan struct{}) doneCh := rf.doneCh[name] rf.unlock() return doneCh } 在节点为协程注册一个 key-value，并返回注册生成的 channel，doneCh。\n此后，在 select 语句中监听 doneCh，收到信号后，立刻退出协程，并执行 rf.deregister()。\nfunc (rf *Raft) deregister(name string) { rf.lock() close(rf.doneCh[name]) delete(rf.doneCh, name) rf.unlock() } 关闭channel，并清除 map 中对应的 key-value。\n当上层调用 Kill() 方法时：\nfunc (rf *Raft) Kill() { atomic.StoreInt32(\u0026amp;rf.dead, 1) rf.lock(\u0026#34;Kill\u0026#34;) defer rf.unlock(\u0026#34;Kill\u0026#34;) for _, ch := range rf.doneCh { go func(ch chan struct{}) { ch \u0026lt;- struct{}{} }(ch) } } 遍历节点维护的 doneCh map，向所有 channel 发送信号，通知其对应的协程立即退出。\n这样就实现了在Kill()被调用时，第一时间主动通知所有后台协程退出，避免占用系统资源。\n接下来是 for 循环中的 select 语句。\ncase \u0026lt;-doneCh: 是刚才介绍的协程退出的通道。 case \u0026lt;-rf.elecTimer.timer.C: 是 electionTimer 超时事件发生的通道。 case \u0026lt;-rf.elecTimer.resetCh: 是 electionTimer 重置事件发生的通道。 需要注意的是，我在这里对 electionTimer 做了一个简单的封装。其拥有一个 reset() 方法。\ntype electionTimer struct { timer *time.Timer resetCh chan struct{} } func (timer *electionTimer) reset() { go func() { timer.resetCh \u0026lt;- struct{}{} }() } 为什么将 electionTimer 设定得这么复杂？按理来说，超时了就开始选举，需要重置的时候直接重置就好。我一开始也是这么想的，然而遇到了一个比较严重的问题。假如将超时事件按照如下处理：\nfunc alerter() { for { select { case \u0026lt;-electionTimer.C: rf.lock() if rf.state == LEADER { rf.unlock() break } rf.state = CANDIDATE rf.startElection() rf.unlock() } } } 假设超时事件发生，程序执行至 rf.lock() ，而此时，节点正在处理 RequestVote RPC，因此 rf.lock() 被阻塞：\nfunc (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { rf.lock(\u0026#34;RequestVote\u0026#34;) defer rf.unlock(\u0026#34;RequestVote\u0026#34;) ... reply.VoteGranted = true reply.Term = rf.currentTerm rf.votedFor = args.CandidateId rf.electionTimer.Reset(randomElectionTimeout()) } 节点将选票投给了另一个 Candidate 节点，退出 RPC handler，然后 alerter 协程成功抢占到了锁——悲剧发生了。刚刚投出选票的节点，立马发起了新一轮的选举。\n这种情况会不会影响系统的 safety？说实话，我暂时还不太清楚。毕竟只是换一个 Leader 而已。但这种情况的确会造成一些测试的 fail，例如发生 split vote，即同时有多个节点 electionTimer 超时时，会使刚刚上任的 Leader 立马变成 Follower，影响了 liveness。而且很显然，这种情况并不是我们希望看到的，我们希望看到的是，要么 electionTimer 超时，发起一轮选举，要么 electionTimer 被重置，选举不会发生。因此我尝试加以解决。\n通过上述分析，可以发现问题的关键在于，超时事件和重置事件不能同时进行，必须互斥进行。因此就有了 alerter 的 select 框架：\nFORLOOP: select { case \u0026lt;-rf.elecTimer.timer.C: rf.lock(\u0026#34;alerter\u0026#34;) if rf.state == LEADER { rf.unlock(\u0026#34;alerter\u0026#34;) break FORLOOP } select { case \u0026lt;-rf.elecTimer.resetCh: rf.elecTimer.reset() rf.unlock(\u0026#34;alerter\u0026#34;) break FORLOOP default: } // start a new election rf.state = CANDIDATE rf.startElection() rf.unlock(\u0026#34;alerter\u0026#34;) case \u0026lt;-rf.elecTimer.resetCh: if !rf.elecTimer.timer.Stop() { select { case \u0026lt;-rf.elecTimer.timer.C: default: } } rf.elecTimer.timer.Reset(randomElectionTimeout()) case \u0026lt;-doneCh: return } 先看重置事件。在封装好的 electionTimer 中，通过调用其 reset 方法将其重置。\nfunc (timer *electionTimer) reset() { timer.resetCh \u0026lt;- struct{}{} } 由于需要重置 electionTimer 时，一般持有锁，而重置 electionTimer 也不需要保证同步，因此这里的 resetCh 使用的是带缓存的 channel，不会阻塞。避免循环等待产生死锁，或发送信号阻塞时间过长，影响系统可用性。\nalerter 监听重置事件。重置事件发生时，对 electionTimer 进行重置。接下来是很经典的 go 重置 timer 的流程：先将 timer stop，假如 stop 时 timer 已经超时，则尝试将 channel 中的信号取出（若信号还未取出的话）。最后再 reset。\nFor a Timer created with NewTimer, Reset should be invoked only on stopped or expired timers with drained channels.\n这样就消除了上述的情况，当 select 语句先进入重置处理，若同时 electionTimer 超时，则将其信号取出，阻止其随后再立刻发起一轮选举。\n再看超时事件。\n如果当前身份已经为 Leader，则忽略超时事件。注意 select 语句中使用 break 的坑。\n随后又有一个 select 语句。这一步的目的是，假如 electionTimer 先超时，进入超时处理，此时 reset 信号来了，则将 reset 信号继续传递，并立刻停止超时处理，再下一次循环中将 electionTimer 重置。若没有 reset 信号，则继续后面的步骤。这样做的原因是，重置步骤是异步进行的，且重置事件与超时事件几乎同时发生时，为了保持 Leader 的 liveness，我们更加偏好优先处理重置事件。毕竟重置的信号已经到了，说明自己已经给其他 Candidate 投了票，或者 Leader 的心跳已经到了，没有必须发起一轮新的选举。\n实际上，这么做也是我的无奈之举。在正常情况下，Leader 发送心跳不会和 Follower 超时同时发生，因为心跳间隔是小于随机超时时间的最小值的。但我的代码有一个诡异的 bug，在一些时候，整个系统（同一时间，所有节点，所以基本可以排除代码阻塞在某处，或者等待 RPC 的问题）可能会同时停顿一段时间（400ms左右），导致 Leader 权力丧失。后面还会尽量详细介绍这个 bug，我有点怀疑是 gc 导致的，不过也实在没有能力继续排查。因此，只能通过偏好重置来增强 Leader 的 liveness。但实际上和我前面介绍的一样，即使没有这个问题，偏好重置也是更合理的选择。\n后面则是发起一轮选举的过程。选举流程相较 Lab2A 有所修改，以下给出代码，就不再做更多的介绍了。\nfunc (rf *Raft) startElection() { rf.currentTerm++ rf.votedFor = rf.me rf.elecTimer.reset() args := RequestVoteArgs{} args.CandidateId = rf.me args.Term = rf.currentTerm args.LastLogIndex = rf.lastLogIndex() args.LastLogTerm = rf.log[rf.lastLogIndex()].Term voteGrantedCnt := 1 // send RequestVote RPCs to all other peers. for i := range rf.peers { if i == rf.me { continue } go func(i int) { reply := RequestVoteReply{} if ok := rf.sendRequestVote(i, \u0026amp;args, \u0026amp;reply); !ok { return } rf.lock(\u0026#34;startElection\u0026#34;) defer rf.unlock(\u0026#34;startElection\u0026#34;) if rf.currentTerm != args.Term || rf.state != CANDIDATE { // outdated reply, or Candidate has been elected as LEADER return } if reply.Term \u0026gt; rf.currentTerm { rf.currentTerm = reply.Term rf.votedFor = -1 rf.state = FOLLOWER rf.elecTimer.reset() return } if !reply.VoteGranted { return } voteGrantedCnt++ if voteGrantedCnt \u0026gt; len(rf.peers)/2 { // gain over a half votes, convert to leader rf.state = LEADER for i := 0; i \u0026lt; len(rf.peers); i++ { // reinitialize upon winning the election rf.nextIndex[i] = rf.lastLogIndex() + 1 rf.matchIndex[i] = 0 } rf.broadcast(true) } }(i) } } 需要注意两点：\n投票是并行异步，前面已经提到过了。需要额外注意的是，各个 voter routines 发送 RPC 使用的 args 要完全一样，在启动 voter routines 前准备好，不可以在 voter routine 内部各自重新加锁读取 args，否则可能会导致发送的 args 不同。未持锁时，任何事情都可能发生。\n在接收到 reply 时，一定要判断一下这是不是过期或无效的 reply，比如当前的 term 已经大于 args 的 term，那么这就是一个过期的 reply。论文中介绍过，对于过期的 reply，直接抛弃即可。Students\u0026rsquo; Guide to Raft 中也提到了这个问题，引用其中的一段话：\nFrom experience, we have found that by far the simplest thing to do is to first record the term in the reply (it may be higher than your current term), and then to compare the current term with the term you sent in your original RPC. If the two are different, drop the reply and return. Only if the two terms are the same should you continue processing the reply. There may be further optimizations you can do here with some clever protocol reasoning, but this approach seems to work well. And not doing it leads down a long, winding path of blood, sweat, tears and despair.\n关于 electionTimer 就介绍到这里。实现最初版本的 electionTimer 逻辑并不困难，但要保证完全地 bug-free (我目前的代码也不能保证)，难度还是很大。其中关于重置和超时同时发生的处理方式，也困扰了我很长时间，最终才得出这个较为稳定的版本。\napplier applier 代码如下：\nfunc (rf *Raft) applier() { doneCh := rf.register(\u0026#34;applier\u0026#34;) defer rf.deregister(\u0026#34;applier\u0026#34;) for { select { case \u0026lt;-rf.applierCh: rf.lock(\u0026#34;applier\u0026#34;) lastApplied := rf.lastApplied rf.lastApplied = rf.commitIndex entries := append([]LogEntry{}, rf.log[lastApplied+1:rf.commitIndex+1]...) rf.unlock(\u0026#34;applier\u0026#34;) for i, entry := range entries { command := entry.Command rf.applyCh \u0026lt;- ApplyMsg{ CommandValid: true, Command: command, CommandIndex: lastApplied + i + 1, } } case \u0026lt;-doneCh: return } } } applier 监听 applierCh，当信号到来时，将 lastApplied 到 commitIndex 间的所有 entry 按序应用至状态机。对于 entry 的 apply，采用一种较懒的方式：在 commitIndex 更新时，向 applierCh 异步发送信号即可。\ngo func() { rf.applierCh \u0026lt;- struct{}{} }() heartbeat heartbeat的代码如下：\nfunc (rf *Raft) heartbeat() { doneCh := rf.register(\u0026#34;heartbeat\u0026#34;) defer rf.deregister(\u0026#34;heartbeat\u0026#34;) for { select { case \u0026lt;-rf.heartbeatTimer.C: rf.lock(\u0026#34;heartbeat\u0026#34;) if rf.state != LEADER { rf.unlock(\u0026#34;heartbeat\u0026#34;) break } rf.broadcast(true) rf.unlock(\u0026#34;heartbeat\u0026#34;) case \u0026lt;-doneCh: return } } } heartbeat 的部分也比较简单。heartbeatTimer 超时后，则 broadcast 一轮心跳信息即可。为什么 heartbeatTimer 不用像 electionTimer 那样制定复杂的规则？本质上是因为 heartbeatTimer 超时和重置的时刻都是已知的，可控的，不像 electionTimer 会并行地随时发生。\nbroadcast 代码如下：\nfunc (rf *Raft) broadcast(isHeartbeat bool) { rf.heartbeatTimer.Stop() rf.heartbeatTimer.Reset(HEARTBEAT_INTERVAL) args := AppendEntriesArgs{} args.LeaderCommit = rf.commitIndex args.LeaderId = rf.me args.Term = rf.currentTerm for i := range rf.peers { if i == rf.me { continue } if isHeartbeat || rf.nextIndex[i] \u0026lt;= rf.lastLogIndex() { go func(i int) { rf.apeChPool[i] \u0026lt;- args }(i) } } } 同样，用于 RPC 的 args 要提前准备好，用 channel 传递给每一个 replicator。需要注意的是，有两种事件会调用 broadcast。\n一是 heartbeatTimer 超时时，此时 Leader 为了维持权力，必须立刻向所有 peer 发送一次 AppendEntries RPC，即使需要同步的 entry 为空（即论文中所说的 heartbeat）。\n二是在上层 client 调用 Start() 函数发送命令时：\nfunc (rf *Raft) Start(command interface{}) (int, int, bool) { rf.lock(\u0026#34;Start\u0026#34;) defer rf.unlock(\u0026#34;Start\u0026#34;) if rf.state != LEADER { return -1, -1, false } index := rf.logLen() + 1 term := rf.currentTerm isLeader := true rf.log = append(rf.log, LogEntry{ Term: term, Command: command, }, ) rf.broadcast(false) return index, term, isLeader } 此时，假如没有需要新同步的 entry，则无需发送一轮空的 AppendEntries RPC。这里的处理参考了 MIT6.824-2021 Lab2 : Raft 的做法。但后来我用 go test cover 的工具简单测试了一下，似乎没有覆盖到无需立即 broadcast 的路径。可能这样的处理是与后续 lab 有关，或者是我的理解有误。（已更新，是我的实现有误）\nreplicator replicator 的代码如下：\nfunc (rf *Raft) replicator(peer int) { doneCh := rf.register(fmt.Sprintf(\u0026#34;replicator%d\u0026#34;, peer)) defer rf.deregister(fmt.Sprintf(\u0026#34;replicator%d\u0026#34;, peer)) for { select { case args := \u0026lt;-rf.apeChPool[peer]: reply := AppendEntriesReply{} rf.rlock(\u0026#34;replicator\u0026#34;) args.PrevLogIndex = rf.nextIndex[peer] - 1 args.PrevLogTerm = rf.log[rf.nextIndex[peer]-1].Term if rf.nextIndex[peer] \u0026lt;= rf.lastLogIndex() { args.Entries = rf.log[rf.nextIndex[peer]:] } rf.runlock(\u0026#34;replicator\u0026#34;) go func() { if ok := rf.sendAppendEntries(peer, \u0026amp;args, \u0026amp;reply); !ok { return } rf.lock(\u0026#34;replicator\u0026#34;) defer rf.unlock(\u0026#34;replicator\u0026#34;) if rf.currentTerm != args.Term || rf.state != LEADER { // outdated reply, or LEADER has no longer been the LEADER return } if reply.Term \u0026gt; rf.currentTerm { rf.currentTerm = reply.Term rf.votedFor = -1 rf.state = FOLLOWER rf.elecTimer.reset() return } if reply.Success { if rf.nextIndex[peer]+len(args.Entries) \u0026gt; rf.lastLogIndex()+1 { // repeated reply, ignore return } rf.nextIndex[peer] = args.PrevLogIndex + len(args.Entries) + 1 rf.matchIndex[peer] = rf.nextIndex[peer] - 1 N := rf.lastLogIndex() for N \u0026gt; rf.commitIndex { if rf.log[N].Term != rf.currentTerm { N-- continue } cnt := 1 for _, matchidx := range rf.matchIndex { if matchidx \u0026gt;= N { cnt++ } } if cnt \u0026lt;= len(rf.peers)/2 { N-- continue } rf.commitIndex = N go func() { rf.applierCh \u0026lt;- struct{}{} }() return } } else { index := -1 found := false for i, entry := range rf.log { if entry.Term == reply.ConflictTerm { index = i found = true } else if found { break } } if found { rf.nextIndex[peer] = index + 1 } else { rf.nextIndex[peer] = reply.ConflictIndex } } }() case \u0026lt;-doneCh: return } } } replicator 也比较复杂。\n由于有多个 replicator 需要注册，在注册是记得根据对应 peer 使用不同的注册名。\nreplicator 监听 broadcast 发送的信号。接收到信号时，向对应 peer 发送 AppendEntries RPC。\n需要注意的是，在接收到 reply 时，如果 reply 已经过期，同样需要直接抛弃。另外，由于 RPC 返回所需的时长不固定，有可能第一个 RPC 还没有返回，第二次心跳已经开始，这时会发送两条相同的 RPC，且都会返回 success（假如 Follower 先处理了第一个 RPC 请求，在处理第二个请求时，log 已经包含了需要同步的 entry，但不会发生冲突）。因此，需要先判断一下 nextIndex 是不是已经被更新过了，假如已经被更新，即 rf.nextIndex[peer]+len(args.Entries) \u0026gt; rf.lastLogIndex()+1，就代表收到了重复的回复，直接抛弃即可。随后则是 Leader 更新其 commitIndex 的流程。\n另外，假如 reply 由于 log 冲突返回了 false，我采用了论文中提到的优化，即 Follower 通过 reply 直接告知 Leader 发生冲突的位置，Leader 不用每次将 nextIndex - 1多次重试。经过测试，这个优化还是挺有必要的，可以显著地缩短 Lab2B 中一项 test 的运行时间。具体方法见 Students\u0026rsquo; Guide to Raft : An aside on optimizations：\nThe Raft paper includes a couple of optional features of interest. In 6.824, we require the students to implement two of them: log compaction (section 7) and accelerated log backtracking (top left hand side of page 8). The former is necessary to avoid the log growing without bound, and the latter is useful for bringing stale followers up to date quickly.\nThese features are not a part of “core Raft”, and so do not receive as much attention in the paper as the main consensus protocol.\nThe accelerated log backtracking optimization is very underspecified, probably because the authors do not see it as being necessary for most deployments. It is not clear from the text exactly how the conflicting index and term sent back from the client should be used by the leader to determine what nextIndex to use. We believe the protocol the authors probably want you to follow is:\nIf a follower does not have prevLogIndex in its log, it should return with conflictIndex = len(log) and conflictTerm = None. If a follower does have prevLogIndex in its log, but the term does not match, it should return conflictTerm = log[prevLogIndex].Term, and then search its log for the first index whose entry has term equal to conflictTerm. Upon receiving a conflict response, the leader should first search its log for conflictTerm. If it finds an entry in its log with that term, it should set nextIndex to be the one beyond the index of the last entry in that term in its log. If it does not find an entry with that term, it should set nextIndex = conflictIndex. A half-way solution is to just use conflictIndex (and ignore conflictTerm), which simplifies the implementation, but then the leader will sometimes end up sending more log entries to the follower than is strictly necessary to bring them up to date.\nRPCs AppendEntries RPC 代码如下：\nfunc (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) { rf.lock(\u0026#34;AppendEntries\u0026#34;) defer rf.unlock(\u0026#34;AppendEntries\u0026#34;) if args.Term \u0026lt; rf.currentTerm { reply.Success = false reply.Term = rf.currentTerm return } if args.Term \u0026gt; rf.currentTerm { rf.currentTerm = args.Term rf.votedFor = -1 } if rf.state != FOLLOWER { rf.state = FOLLOWER } rf.elecTimer.reset() if args.PrevLogIndex \u0026gt; rf.lastLogIndex() || args.PrevLogIndex \u0026gt; 0 \u0026amp;\u0026amp; rf.log[args.PrevLogIndex].Term != args.PrevLogTerm { // Reply false if log doesn\u0026#39;t contain an entry at prevLogIndex whose term matches prevLogTerm reply.Success = false reply.Term = rf.currentTerm // accelerated log backtracking optimization if args.PrevLogIndex \u0026gt; rf.lastLogIndex() { reply.ConflictTerm = -1 reply.ConflictIndex = rf.lastLogIndex() + 1 } else { reply.ConflictTerm = rf.log[args.PrevLogIndex].Term index := args.PrevLogIndex - 1 for index \u0026gt; 0 \u0026amp;\u0026amp; rf.log[index].Term == reply.ConflictTerm { index-- } reply.ConflictIndex = index + 1 } return } // If an existing entry conflicts with a new one (same index but different terms), // delete the existing entry and all that follow it for i, entry := range args.Entries { index := args.PrevLogIndex + i + 1 if index \u0026gt; rf.lastLogIndex() || rf.log[index].Term != entry.Term { rf.log = rf.log[:index] rf.log = append(rf.log, append([]LogEntry{}, args.Entries[i:]...)...) break } } if args.LeaderCommit \u0026gt; rf.commitIndex { // If leaderCommit \u0026gt; commitIndex, set commitIndex = min(leaderCommit, index of last new entry) rf.commitIndex = min(args.LeaderCommit, rf.logLen()) go func() { rf.applierCh \u0026lt;- struct{}{} }() } reply.Success = true reply.Term = rf.currentTerm } AppendEntries RPC 没有太多可说的，严格按照 Figure 2 来就好。另外注意这里也需要实现前面说的 accelerated log backtracking optimization。\nRequestVote RPC 代码如下：\nfunc (rf *Raft) RequestVote(args *RequestVoteArgs, reply *RequestVoteReply) { rf.lock(\u0026#34;RequestVote\u0026#34;) defer rf.unlock(\u0026#34;RequestVote\u0026#34;) if args.Term \u0026lt; rf.currentTerm { reply.VoteGranted = false reply.Term = rf.currentTerm return } if args.Term \u0026gt; rf.currentTerm { rf.currentTerm = args.Term rf.votedFor = -1 if rf.state != FOLLOWER { rf.state = FOLLOWER rf.elecTimer.reset() } } if rf.votedFor != -1 \u0026amp;\u0026amp; rf.votedFor != args.CandidateId { reply.VoteGranted = false reply.Term = rf.currentTerm return } if rf.logLen() \u0026gt; 0 { rfLastLogTerm := rf.log[rf.lastLogIndex()].Term rfLastLogIndex := rf.lastLogIndex() if rfLastLogTerm \u0026gt; args.LastLogTerm || rfLastLogTerm == args.LastLogTerm \u0026amp;\u0026amp; rfLastLogIndex \u0026gt; args.LastLogIndex { // If candidate\u0026#39;s log is at least as up-to-date as receiver\u0026#39;s log, grant vote; otherwise reject reply.VoteGranted = false reply.Term = rf.currentTerm return } } reply.VoteGranted = true reply.Term = rf.currentTerm rf.votedFor = args.CandidateId rf.elecTimer.reset() } 同样也没有说明可说的。按照 Figure 2 来。\nLab2B 的全部实现大致就是这样。回过头来看好像也不是特别复杂，但确实折磨了我很久，看了整整几天的 log。\nHappy Debugging 前面也提到了，对于 Lab2B，实现一版能够通过一次 test，甚至能够通过90%百次 test 的代码并不是特别难，严格按照 Figure2 编写就好。然而，前面 90% 的任务需要 90% 的时间完成，后面 10% 的任务需要另一个 90% 的时间来完成（甚至更久）。\n关于 debug，由于 Raft 是个多线程的项目，也有 RPC，因此以往打断点，单步 debug 的方式肯定行不通。实际上，最原始的方法就是最有效的方法，print log。\n在编写代码前，强烈推荐阅读 Debugging by Pretty Printing。这篇博客详细教你如何打日志，并用 python 的 rich 库打印漂亮工整的命令行输出。在编写代码时，记得在关键处增加一些 Debug 语句，时刻掌握系统变化的情况。\n这样各节点的状态清晰很多，方便 debug。\n同时，在 Lab2B 中，我也遭遇了 Lab2A 中没有碰到的死锁问题。为了追踪锁的使用情况，我对锁做了一点封装。\nfunc (rf *Raft) lock(where string) { Debug(dLock, \u0026#34;S%d locked %s\u0026#34;, rf.me, where) rf.mu.Lock() } func (rf *Raft) unlock(where string) { Debug(dLock, \u0026#34;S%d unlocked %s\u0026#34;, rf.me, where) rf.mu.Unlock() } 主要就是增加了一个 debug 语句，对锁的使用情况进行跟踪。有了锁的日志之后，解决死锁问题不算困难。一般都是在同一段代码中不小心锁了两次，或者在发送、接收阻塞 channel 时持有了锁。比较好排查。\nA Confusing Bug 截至 7.19，我已经跑了上万次 Lab2B 的 test。其中仍会出现几次 fail。经过排查，全部都是同一种原因导致，即上文提到过的，整个系统偶尔会出现一次400ms左右的停顿，导致 Leader 失去权力，出现新的 Candidate 并竞选成为 Leader。其中一次日志如下：\n这是 Lab2B test 中最简单的 BasicAgreeTest，内容是成功选出 Leader，然后同步3条 entry 即可。正常情况下不会出现 Leader 变动。\n日志中每条语句前的时间戳单位为 ms。此时 S2 为 Leader。\n可以看到，在 730 ms 时，S2进行了一次心跳，成功将 {1，300} （任期号为1，命令内容为300）同步给 S0 和 S1。我设置的心跳间隔为 150 ms。然而，在 150 ms 后，也就是 880 ms 时，系统没有任何动作。此时 S2 本应该发送一次心跳，告知 S0 和 S1 {1，300} 已提交，可以将其应用至状态机。\n在 731 ms 时，S0 的 electionTimer 被 reset 为 308 ms，S1 的 electionTimer 被 reset 为 331 ms。按理说，即使没有接收到 Leader 的心跳，S0 也会在 308 ms 后，也就是 1039 ms 时，S0 electionTimer 超时，发起一轮选举。然而此时系统仍然没有动作。\n在 1092 ms 时，所有节点似乎同时苏醒了，S0 和 S1 都发起了一轮选举，S2 也接收到了投票请求。于是 S1 成功当选 Leader。\nS1 在当选 Leader 时，所有节点的 log 都是一致的，为 {1，100} {1，200} {1，300}，其中第3条 entry 没有提交。而由于 Raft 的特性，Leader 不会提交不属于其当前任期的 entry，只会在成功同步并提交下一条到来的 entry 时，间接地将第3条 entry 提交。然而不幸的是，第3条 entry 已经是 BasicAgree 会发送的最后一条 entry。因此，在这次 test 中，这条 entry 无法被提交，也就导致了 test fail。\n系统出现诡异停顿的原因是什么？是 timer 的种种坑，还是 gc 的锅？原谅我实在没有能力排查，因为这种 bug 出现的概率极低（应该略高于导致 test fail 的频率，因为可能在出现这种 bug 时，test 仍可以通过，导致 bug 被吞掉），并且这种 bug 实际上可以看成是所有节点同时 down 掉几百毫秒，对于 Raft 系统来说应该是可以容忍的，可能只会造成一次 Leader 更替。导致 test fail 的直接原因是 Raft 不直接提交不属于当前任期 entry 的特性，和 test 刚好没有后续需要同步的 entry。\n这里就留下一个遗憾吧，希望我以后有能力排查，到底是哪里出了问题。\nSummary 做完 Lab2B 的感觉很爽，但过程也真的很痛苦。从自信地通过第一次 test，到好几个痛苦 debug 的深夜，再到最后的成功实现。有种便秘的酣畅淋漓的感觉（？）。Lab2A 和 Lab2B 是 Raft 算法的核心内容，能够成功撸下来还是有一点点小小的成就感的。\n另外想说的是。6.824 Guidance 中提到了，对于计时操作，不要使用 go timer 或者 ticker，而是应该使用 sleep，在醒来时检查各种变量来实现。然而我还是硬着头皮用了 timer，毕竟这样更加直观，或许也更优雅。然而我不知道这是不是一个正确的选择。因为 timer 的各种诡异现象 debug 到破防的时候，我也想过是不是该推倒重来，全部换成 sleep。最终翻了很多篇博客和资料，还是勉强做出了这个能用的版本。是不是真的用 sleep 更容易实现呢？我也不知道。也许选择比努力更重要，但还是要坚持自己的选择，继续努力吧。\nLab2C Raft Persistence 对 lab2C，guidance 中给出了 hard 的难度，但实际上只要认真完成了 lab2B，lab2C 应该是 easy。\nRaft 节点挂掉后，在重新恢复时，会从 disk 中读取其此前的状态。Figure 2 中已经告诉我们哪些状态需要持久化：\ncurrentTerm votedFor log 因此，只要我们对这些变量进行了修改，就需要进行一次持久化，将这些变量记录在 disk 上。（实际上，这样可能对性能有所影响，因为写入 disk 的 IO 操作比较耗时，但对 6.824 来说这样的简单处理没有问题）\n在 lab2C 中，我们不需要真的将状态写入 disk，为了简化代码和方便测试，lab2C 提供了给我们一个 persister 类。在需要对状态进行持久化时，使用 Raft 初始化时传入的 persister 对象对其存储即可。读取状态时也从 persister 中读取。\n需要实现 persist() and readPersist() 两个函数：\nfunc (rf *Raft) persist() { w := new(bytes.Buffer) e := labgob.NewEncoder(w) e.Encode(rf.currentTerm) e.Encode(rf.votedFor) e.Encode(rf.log) data := w.Bytes() rf.persister.SaveRaftState(data) } func (rf *Raft) readPersist(data []byte) { if data == nil || len(data) \u0026lt; 1 { // bootstrap without any state? return } r := bytes.NewBuffer(data) d := labgob.NewDecoder(r) var currentTerm int var votedFor int var log []LogEntry if d.Decode(\u0026amp;currentTerm) != nil || d.Decode(\u0026amp;votedFor) != nil || d.Decode(\u0026amp;log) != nil { panic(\u0026#34;readPersist decode fail\u0026#34;) } rf.currentTerm = currentTerm rf.votedFor = votedFor rf.log = log } 之后在 Raft 改变 currentTerm、votedFor 或 log 时，及时调用 rf.persist() 即可。\nLab2D Raft Log Compaction Lab2D 编写初版 pass 代码大概用了两小时，随后断断续续 debug 修改好几天。\nLab2D 是实现日志压缩。这其实并不是 Raft 算法的核心部分，Raft 算法的强一致性等特性也不依赖于日志压缩。但由于内存容量的限制，日志压缩在 Raft 的工业实现上也很重要。此前我们将 log 全部存储在内存之中，但随着 log 的不断增长，内存总是会被消耗殆尽。因此及时地将过老的已提交的日志压缩成快照很有必要。\nDesign Raft 的 snapshot 包含三个字段：\nstate machine state: 即状态机的快照。假如节点崩溃，随后重放 log 恢复时，以快照为初始状态进行重放。\nlast included index: 快照包含的最后一个 entry 的 index。这个字段是 log index 和 real index 的偏移量。log index 指某个 entry 在当前内存中 log 的位置，real index 则是某个 entry 的全局 index。例如如下情况：\nreal index = last included index + log index\nlast included term: 即 last included index 对应 entry 的 term。用于 AppendEntries RPC 中对于 prevLogIndex 和 prevLogTerm 的检测。\n快照可以由上层应用触发。当上层应用认为可以将一些已提交的 entry 压缩成 snapshot 时，其会调用节点的 Snapshot()函数，将需要压缩的状态机的状态数据传递给节点，作为快照。\n在正常情况下，仅由上层应用命令节点进行快照即可。但如果节点出现落后或者崩溃，情况则变得更加复杂。考虑一个日志非常落后的节点 i，当 Leader 向其发送 AppendEntries RPC 时，nextIndex[i] 对应的 entry 已被丢弃，压缩在快照中。这种情况下， Leader 就无法对其进行 AppendEntries。取而代之的是，这里我们应该实现一个新的 RPC，将 Leader 当前的快照直接发送给非常落后的 Follower。\n在 Raft 论文中，InstallSnapshot RPC 将 snapshot 分块发送，但在 6.824 的实现中，我们直接将整个 snapshot 全部一次发送即可。需要实现的字段如下：\nArgs\nterm Leader 的任期。同样，InstallSnapshot RPC 也要遵循 Figure 2 中的规则。如果节点发现自己的任期小于 Leader 的任期，就要及时更新。 leaderId 用于重定向 client 。 lastIncludedindex \u0026amp; lastIncludedTerm 快照中包含的最后一个 entry 的 index 和 term。 data[] 快照数据。 Reply\nterm 节点的任期。Leader 发现高于自己任期的节点时，更新任期并转变为 Follower。 Receiver Implementation\n如果 term \u0026lt; currentTerm，直接返回。 保存 snapshot。 如果当前 log 中包含 index 和 term 与 last included entry 相同的 entry，则保留此 entry 后的 log，并返回。 丢弃所有 log。 使用 snapshot 重置状态机。 整个日志压缩的设计就是这样，并不算复杂。但实现起来还是有点麻烦，有特别多的 corner case，且需要对之前的代码进行大量改动。\nImplementation index mapping 引入日志压缩后的一大改变是，访问 log 不像之前那么轻松了。\nRaft 论文中约定，log 的索引从 1 开始。在此前的实现中，使用数组保存 log，直接使用索引访问即可。但在引入日志压缩后，需要配合 lastIncludedIndex 进行换算。\n// get an entry with its real index func (rf *Raft) entry(index int) LogEntry { return rf.log[index-rf.lastIncludedIndex] } // switch real index to log index func (rf *Raft) logIndex(realIndex int) int { return realIndex - rf.lastIncludedIndex } // switch log index to real index func (rf *Raft) realIndex(logIndex int) int { return logIndex + rf.lastIncludedIndex } // get the real index of the latest entry in log func (rf *Raft) lastLogIndex() int { return len(rf.log) - 1 + rf.lastIncludedIndex } // get the term of the latest entry in log func (rf *Raft) lastLogTerm() int { // log[0] : last included entry in snapshot return rf.log[len(rf.log)-1].Term } 在这里我实现了一系列的函数，方便索引的换算。其中，real index 指 entry 本身的索引，也就是从 1 递增的索引。log index 指 entry 存放在内存 log 中的位置。\n另外，由于索引从 1 开始，log[0] 的位置始终空出。因此可以将 last included entry 存放在 log[0] 的位置，这样在检测前置 entry 等操作中，需要访问 last included entry 时无需做额外的判断，算是一个小 trick。\n此后，将之前所有访问 log 的操作全部用新接口替换。\nSnapshot() func (rf *Raft) Snapshot(index int, snapshot []byte) { rf.lock(\u0026#34;Snapshot\u0026#34;) defer rf.unlock(\u0026#34;Snapshot\u0026#34;) defer rf.persist() if index \u0026lt;= rf.lastIncludedIndex { // outdated request return } rf.snapshot = snapshot rf.lastIncludedTerm = rf.entry(index).Term rf.lastIncludedIndex = index rf.log = append([]LogEntry{{Term: rf.lastIncludedTerm}}, rf.log[rf.logIndex(index)+1:]...) } Snapshot() 方法由上层调用，代表上层已经成功将给定 index 及其前的 entry 进行了压缩，压缩结果为 snapshot。因此，节点需要丢弃已被压缩的日志。\n需要注意的是，在抛弃日志时，不能简单地截取切片，例如\nlog = log[index+1:] 这和 go slice 的底层实现有关。go slice 的底层是一个定长数组和一个给定的引用范围。在数组容量不足时，会自动扩容，此时是在不同的地址创建了一个新的数组。在截取 slice 时，实际上没有创建新的数组，只是改变了引用的范围。但比较坑的是，即使此后仅会使用某个范围内的元素，整个数组也不会被 gc 回收，而是一直保留，最后造成 oom。\n因此，在抛弃日志时，可以用 append 方法创建新数组，确保之前的底层数组会被 gc 回收。\nInstallSnapshot RPC func (rf *Raft) InstallSnapshot(args *InstallSnapshotArgs, reply *InstallSnapshotReply) { rf.lock(\u0026#34;InstallSnapshot\u0026#34;) defer rf.unlock(\u0026#34;InstallSnapshot\u0026#34;) if args.Term \u0026lt; rf.currentTerm { reply.Term = rf.currentTerm return } defer rf.persist() if args.Term \u0026gt; rf.currentTerm { rf.currentTerm = args.Term rf.votedFor = -1 } if rf.state != FOLLOWER { rf.state = FOLLOWER } rf.elecTimer.reset() reply.Term = rf.currentTerm rf.snapshot = args.Data if args.LastIncludedIndex \u0026lt; rf.lastIncludedIndex { return } if rf.lastIncludedIndex == args.LastIncludedIndex \u0026amp;\u0026amp; rf.lastIncludedTerm == args.LastIncludedTerm { return } defer func() { msg := ApplyMsg{ CommandValid: false, SnapshotValid: true, Snapshot: args.Data, SnapshotTerm: args.LastIncludedTerm, SnapshotIndex: args.LastIncludedIndex, } go func() { rf.applyCh \u0026lt;- msg }() }() for i := 1; i \u0026lt; len(rf.log); i++ { if rf.realIndex(i) == args.LastIncludedIndex \u0026amp;\u0026amp; rf.log[i].Term == args.LastIncludedTerm { rf.lastIncludedTerm = args.LastIncludedTerm rf.lastIncludedIndex = args.LastIncludedIndex rf.log = append([]LogEntry{{Term: rf.lastIncludedTerm}}, rf.log[i+1:]...) rf.lastApplied = rf.lastIncludedIndex rf.commitIndex = rf.lastIncludedIndex return } } rf.lastIncludedIndex = args.LastIncludedIndex rf.lastIncludedTerm = args.LastIncludedTerm rf.log = []LogEntry{{Term: rf.lastIncludedTerm}} rf.lastApplied = rf.lastIncludedIndex rf.commitIndex = rf.lastIncludedIndex } 在实现 InstallSnapshot RPC 时，也要遵守之前 Figure 2 中的规则。另外，InstallRPC 也可以看成 Leader 的一次心跳，Follower 既然接收到了来自 Leader 的 RPC，就代表 Leader 还“活着”。\n在最后，需要更新节点的 lastApplied 和 commitIndex。生成快照实际上是将 entry 应用至状态机。我在这里简单地将lastApplied 和 commitIndex 直接设置为 lastIncludedIndex。实际上对于 commitIndex 的更新应该存在更合理的方式。\nLeader Broadcast 引入日志压缩后，Broadcast 也需要做一些修改，部分情况需要调用 InstallSnapshot RPC 而不是 AppendEntries RPC。同时，我发现了之前 Broadcast 模型存在的问题，在这里一并说明。\nfunc (rf *Raft) broadcast(isHeartbeat bool) { if isHeartbeat { rf.heartbeatTimer.Stop() rf.heartbeatTimer.Reset(HEARTBEAT_INTERVAL) } info := replicateInfo{ term: rf.currentTerm, leaderCommit: rf.commitIndex, } for i := range rf.peers { if i == rf.me { continue } if isHeartbeat { go rf.replicate(i, info) continue } if rf.nextIndex[i] \u0026lt;= rf.lastLogIndex() { go func(peer int) { rf.replicateCh[peer] \u0026lt;- info }(i) } } } Broadcast 存在两种行为，一种为需要立即发送 RPC 维持权力的心跳，另一种则是不那么紧急的 replicate 请求。heartbeatTimer 到期时，立即发送心跳；上层传入新 entry 时，则发送不紧急的 replicate 请求。\n需要立即发送心跳时，直接并行地调用 replicate 函数。\nfunc (rf *Raft) replicate(peer int, info replicateInfo) { rf.rlock(\u0026#34;replicate\u0026#34;) if rf.nextIndex[peer] \u0026gt; rf.lastIncludedIndex { // nextIndex is located in log args := AppendEntriesArgs{ Term: info.term, LeaderId: rf.me, PrevLogIndex: rf.nextIndex[peer] - 1, PrevLogTerm: rf.entry(rf.nextIndex[peer] - 1).Term, LeaderCommit: info.leaderCommit, } reply := AppendEntriesReply{} if rf.nextIndex[peer] \u0026lt;= rf.lastLogIndex() { args.Entries = rf.log[rf.logIndex(rf.nextIndex[peer]):] } rf.runlock(\u0026#34;replicate\u0026#34;) rf.doAppendEntries(peer, \u0026amp;args, \u0026amp;reply) } else { // nextIndex is located in snapshot args := InstallSnapshotArgs{ Term: info.term, LeaderId: rf.me, LastIncludedIndex: rf.lastIncludedIndex, LastIncludedTerm: rf.lastIncludedTerm, Data: rf.snapshot, } reply := InstallSnapshotReply{} rf.runlock(\u0026#34;replicate\u0026#34;) rf.doInstallSnapshot(peer, \u0026amp;args, \u0026amp;reply) } } 可以看到，replicate 也存在两种行为。当需要同步的 entry 位于 log 中时，发送 AppendEntries RPC 即可；当 Follower 过于落后，需要同步的 entry 位于 snapshot 中时，则发送 InstallSnapshot RPC。\n当上层调用 Start() 触发 Broadcast 时，则向 rf.replicateCh[peer] 发送信号，通过常驻在后台的 n-1 个 replicator 协程进行同步。\nfunc (rf *Raft) replicator(peer int) { doneCh := rf.register(fmt.Sprintf(\u0026#34;replicator%d\u0026#34;, peer)) defer rf.deregister(fmt.Sprintf(\u0026#34;replicator%d\u0026#34;, peer)) for { select { case info := \u0026lt;-rf.replicateCh[peer]: replicateDoneCh := make(chan struct{}) go func() { // ignore redundant replicating request for { select { case \u0026lt;-rf.replicateCh[peer]: case \u0026lt;-replicateDoneCh: return } } }() rf.replicate(peer, info) // blocked by RPC replicateDoneCh \u0026lt;- struct{}{} case \u0026lt;-doneCh: return } } } replicator 接收到来自 rf.replicateCh[peer] 的信号后，会调用 replicate 进行一次同步。replicate 方法是阻塞的，会等待 RPC 返回 reply 后才会返回。在调用 RPC 前，replicator 会起一监听 goroutine，监听并抛弃此后重复的 replicate 请求。并在 RPC 处理完成后关闭这个 goroutine。因此，在上层短期连续调用多次 Start() 方法并调用 rf.broadcast(fasle) 时，若此前的 RPC 还没有返回，则这次的 Broadcast 请求会被抛弃，不再发送 RPC。这样做避免了短期内发送大量包含重复 entry 的 RPC，节省了带宽。\n在此前的实现中，短期连续调用 Start() 时，仍会多次发送 RPC。这里进行了更正。\nSummary 到这里，6.824 Lab2 的所有部分全部实现。实现和 debug 时间加起来大概花了10天左右，其余的时间就在写写记录，摸摸鱼。整体做下来还是成就感满满的。体感上最大的困难在于日复一日地对着上万行的日志找出难以察觉的错误，以及努力复现那些概率极低的边界情况。debug 时间还是太长了，稍微有点消磨斗志，到最后也不知道什么时候是个头。我在千次测试全部 pass 之后就没有再进行更多的测试了，希望我写的这个简陋的 Raft 在后续 lab 中不会出什么岔子。\n继续前进吧。\n","permalink":"https://blog.eleven.wiki/posts/mit6.824-lab2-raft/","summary":"趁着暑假有空，把鸽了很久的 MIT6.824 做一下。Lab1 是实现一个 Map-Reduce，因为和 Raft 主线关系不大（因为懒），就略过了。另外，这次尝试实现一个 part 就来记录相关的内容，以免在全部实现后忘记部分细节（以免之后太懒不想写）。因此，不同 part 的代码会变化，请以最终版本的代码为准（但保证每一 part 的代码可以正常通过绝大部分相应的测试）。同时，在写下某一 part 的记录时，我对 Raft 的整体把握也难免有所不足。\nResources Course\u0026rsquo;s Page 课程主页 Students\u0026rsquo; Guide to Raft 一篇引导博客 Debugging by Pretty Printing debug 技巧，强烈推荐阅读和运用 Raft Q\u0026amp;A 关于 Raft 的一些 Q\u0026amp;A Raft Visualization Raft 动画演示 In Search of an Understandable Consensus Algorithm Raft 论文 Lab2A Raft Leader Election Lab2A 实现时间为6.22~6.24。\nLab2A 主要实现 Raft 的选主过程，包括选举出 Leader 和 Leader 通过心跳维持身份。\nDesign 首先是选主过程的状态机模型：\n接下来是 Raft 论文中最为重要的 Figure 2:","title":"MIT6.824 Lab2 Raft"},{"content":"First time here, just a test. 测试。\n","permalink":"https://blog.eleven.wiki/posts/hugo-test/","summary":"First time here, just a test. 测试。","title":"Hugo Test"}]